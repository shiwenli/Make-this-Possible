{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code written and working on simulated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PhiGamma(alpha, beta, w, T):\n",
    "    k = len(alpha)\n",
    "    N = len(w)\n",
    "    alpha = np.array(alpha)\n",
    "    beta = np.array(beta)\n",
    "    gamma = np.tile(alpha + N/k, ((T+1),1))\n",
    "    phi = np.array([[[1/k]*k]*N]*(T+1))\n",
    "    for t in range(T):\n",
    "        for n in range(N):\n",
    "            for i in range(k):\n",
    "                phi[t+1,n,i] = beta[i,n] * np.exp(sp.special.psi(gamma[t,i])) #beta[i,n], n should be w_n.\n",
    "            phi_const = np.sum(phi[t+1,n,])\n",
    "            phi[t+1,n,] = phi[t+1,n,]/phi_const\n",
    "        gamma[t+1,] = alpha + np.sum(phi[t+1,], axis=0)\n",
    "    return(gamma[T,],phi[T,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.66666667,  1.66666667,  1.66666667]),\n",
       " array([[ 0.33333333,  0.33333333,  0.33333333],\n",
       "        [ 0.33333333,  0.33333333,  0.33333333],\n",
       "        [ 0.33333333,  0.33333333,  0.33333333],\n",
       "        [ 0.33333333,  0.33333333,  0.33333333]]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test function\n",
    "N=4\n",
    "T=5\n",
    "alpha = np.array([1/3,1/3,1/3])\n",
    "beta = np.array([[1/2,1/2,1/2,1/2,1/2,1/2], [1/2,1/2,1/2,1/2,1/2,1/2], [1/2,1/2,1/2,1/2,1/2,1/2]])\n",
    "w=0\n",
    "a = np.stack((beta,beta))\n",
    "PhiGamma(alpha, beta, w, N, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_step(self, parsed_corpus=None, local_parameter_iteration=50, local_parameter_converge_threshold=1e-6):\n",
    "        if parsed_corpus == None:\n",
    "            word_ids = self._parsed_corpus[0];\n",
    "            word_cts = self._parsed_corpus[1];\n",
    "        else:\n",
    "            word_ids = parsed_corpus[0]\n",
    "            word_cts = parsed_corpus[1];\n",
    "        \n",
    "        assert len(word_ids) == len(word_cts);\n",
    "        number_of_documents = len(word_ids);\n",
    "        \n",
    "        document_log_likelihood = 0;\n",
    "        words_log_likelihood = 0;\n",
    "\n",
    "        # initialize a V-by-K matrix phi sufficient statistics\n",
    "        phi_sufficient_statistics = np.zeros((self._number_of_topics, self._number_of_types));\n",
    "        \n",
    "        # initialize a D-by-K matrix gamma values\n",
    "        gamma_values = np.zeros((number_of_documents, self._number_of_topics)) + self._alpha_alpha[np.newaxis, :] + 1.0 * self._number_of_types / self._number_of_topics;\n",
    "        \n",
    "        E_log_eta = compute_dirichlet_expectation(self._eta);\n",
    "        assert E_log_eta.shape == (self._number_of_topics, self._number_of_types);\n",
    "        if parsed_corpus != None:\n",
    "            E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "        \n",
    "        # iterate over all documents\n",
    "        # for doc_id in xrange(number_of_documents):\n",
    "        for doc_id in np.random.permutation(number_of_documents):\n",
    "            # compute the total number of words\n",
    "            # total_word_count = self._corpus[doc_id].N()\n",
    "            total_word_count = np.sum(word_cts[doc_id]);\n",
    "\n",
    "            # initialize gamma for this document\n",
    "            gamma_values[doc_id, :] = self._alpha_alpha + 1.0 * total_word_count / self._number_of_topics;\n",
    "            \n",
    "            # term_ids = np.array(self._corpus[doc_id].keys());\n",
    "            # term_counts = np.array([self._corpus[doc_id].values()]);\n",
    "            term_ids = word_ids[doc_id];\n",
    "            term_counts = word_cts[doc_id];\n",
    "            assert term_counts.shape == (1, len(term_ids));\n",
    "\n",
    "            # update phi and gamma until gamma converges\n",
    "            for gamma_iteration in xrange(local_parameter_iteration):\n",
    "                assert E_log_eta.shape == (self._number_of_topics, self._number_of_types);\n",
    "                log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[[doc_id], :]), (word_ids[doc_id].shape[0], 1));\n",
    "                assert log_phi.shape == (len(term_ids), self._number_of_topics);\n",
    "                log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis];\n",
    "                assert log_phi.shape == (len(term_ids), self._number_of_topics);\n",
    "                \n",
    "                gamma_update = self._alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(term_counts.transpose())), axis=0));\n",
    "                \n",
    "                mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]));\n",
    "                gamma_values[doc_id, :] = gamma_update;\n",
    "                if mean_change <= local_parameter_converge_threshold:\n",
    "                    break;\n",
    "            \n",
    "            # Note: all terms including E_q[p(\\theta | \\alpha)], i.e., terms involving \\Psi(\\gamma), are cancelled due to \\gamma updates in E-step\n",
    "            \n",
    "            # compute the alpha terms\n",
    "            document_log_likelihood += sp.special.gammaln(np.sum(self._alpha_alpha)) - np.sum(sp.special.gammaln(self._alpha_alpha))\n",
    "            # compute the gamma terms\n",
    "            document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]));\n",
    "            # compute the phi terms\n",
    "            document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi));\n",
    "            \n",
    "            # Note: all terms including E_q[p(\\eta | \\beta)], i.e., terms involving \\Psi(\\eta), are cancelled due to \\eta updates in M-step\n",
    "            if parsed_corpus != None:\n",
    "                # compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step during training\n",
    "                words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids]);\n",
    "            \n",
    "            assert(log_phi.shape == (len(term_ids), self._number_of_topics));\n",
    "            phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T;\n",
    "            \n",
    "            if (doc_id + 1) % 1000 == 0:\n",
    "                print \"successfully processed %d documents...\" % (doc_id + 1);\n",
    "        \n",
    "        if parsed_corpus == None:\n",
    "            self._gamma = gamma_values;\n",
    "            return document_log_likelihood, phi_sufficient_statistics\n",
    "        else:\n",
    "            return words_log_likelihood, gamma_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def m_step(self, phi_sufficient_statistics):\n",
    "        # Note: all terms including E_q[p(\\eta|\\beta)], i.e., terms involving \\Psi(\\eta), are cancelled due to \\eta updates\n",
    "            \n",
    "        # compute the beta terms\n",
    "        topic_log_likelihood = self._number_of_topics * (sp.special.gammaln(np.sum(self._alpha_beta)) - np.sum(sp.special.gammaln(self._alpha_beta)));\n",
    "        # compute the eta terms\n",
    "        topic_log_likelihood += np.sum(np.sum(sp.special.gammaln(self._eta), axis=1) - sp.special.gammaln(np.sum(self._eta, axis=1)));\n",
    "        \n",
    "        self._eta = phi_sufficient_statistics + self._alpha_beta;\n",
    "        assert(self._eta.shape == (self._number_of_topics, self._number_of_types));\n",
    "        \n",
    "        # self._E_log_eta = compute_dirichlet_expectation(self._eta);\n",
    "        \n",
    "        # compute the sufficient statistics for alpha and update\n",
    "        alpha_sufficient_statistics = sp.special.psi(self._gamma) - sp.special.psi(np.sum(self._gamma, axis=1)[:, np.newaxis]);\n",
    "        alpha_sufficient_statistics = np.sum(alpha_sufficient_statistics, axis=0);  # [np.newaxis, :];\n",
    "        \n",
    "        return topic_log_likelihood, alpha_sufficient_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background Introduction\n",
    "### A. Question interested\n",
    "Due to the Internet, information retrieval has exploded. For a range of reasons in the modern world, people want to compare and contrast two documents for structure, text, words, topics. The goal is to retrieve information from a large text corpus. “Thus each word is generated from a single topic, and different words in one document can be generated by different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the ‘reduced description’ associated with the document”. \n",
    "### B. Basic thoughts and procedure\n",
    "A significant step forward in this regard was made by modeling each word in a document as a sample from a mixture model, where the mixture components are random variables that can be viewed as representations of “topics”. The solution to the goal is the Latent Dirichlet Allocation (LDA) model, a three-level probabilistic model in natural language processing for comparing and contrasting collections of discrete data, and providing a short description of the statistical relationships in those large discrete data sets. In LDA, each document is composed of various topics with Dirichlet prior distribution. Each topic has a probability of generating its corresponding word and those words that do not belong to any topic have an even probability of being placed into each category.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Algorithm \n",
    "### A. Models\n",
    "#### (1). Notation\n",
    "Formally, we define the following terms:\n",
    "- A _word_ is the basic unit of discrete data, defined to be an item from a vocabulary indexed by {1, . . . , V }. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the _v_th word in the vocabulary is represented by a V -vector w such that $w^v = 1$ and $w^u = 0$ for $u\\neq v$.\n",
    "- _A _document_ is a sequence of N words denoted by $\\textbf{w} = (w_1,w_2,... ,w_N)$, where $w_n$ is the _n_ th word in the sequence.\n",
    "- A _corpus_ is a collection of M documents denoted by $D = \\{\\textbf{w}_1,\\textbf{w}_2,... ,\\textbf{w}_M \\}$.\n",
    "\n",
    "#### (2). Latent Dirichlet allocation\n",
    "LDA assumes the following generative process for each document $\\textbf{w}$ in a corpus D:\n",
    "1. Choose N $\\sim$ Poisson(ξ).\n",
    "2. Chooseθ $\\sim$ Dir(α).\n",
    "3. For each of the N words $w_n$:\n",
    " - Choose a topic $z_n$ $\\sim$ Multinomial(θ).\n",
    " - Choose a word $w_n$ from p($w_n$ |$z_n$,β), a multinomial probability conditioned on the topic $z_n$.\n",
    "\n",
    "### B. Techniques for inference\n",
    "#### (1). Variational inference\n",
    "\n",
    "#### (2). Parameter estimation\n",
    "The derivation of the variational EM algorithm for LDA yields the following iterative algorithm:\n",
    "  1. (E-step) For each document, find the optimizing values of the variational parameters {$γ^*_d , φ^*_d : d \\in D$}. This is done as described in the previous section.\n",
    "  2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters α and β. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n",
    "\n",
    "## 3. Application\n",
    "### A. Simulated data\n",
    "### B. Real dataset\n",
    "## 4. Optimization and Analysis\n",
    "## 5. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
