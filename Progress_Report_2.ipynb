{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code written and working on simulated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PhiGamma(alpha, beta, w, T):\n",
    "    k = len(alpha)\n",
    "    N = len(w)\n",
    "    alpha = np.array(alpha)\n",
    "    beta = np.array(beta)\n",
    "    gamma = np.tile(alpha + N/k, ((T+1),1))\n",
    "    phi = np.array([[[1/k]*k]*N]*(T+1))\n",
    "    for t in range(T):\n",
    "        for n in range(N):\n",
    "            for i in range(k):\n",
    "                phi[t+1,n,i] = beta[i,n] * np.exp(sp.special.psi(gamma[t,i])) #beta[i,n], n should be w_n.\n",
    "            phi_const = np.sum(phi[t+1,n,])\n",
    "            phi[t+1,n,] = phi[t+1,n,]/phi_const\n",
    "        gamma[t+1,] = alpha + np.sum(phi[t+1,], axis=0)\n",
    "    return(gamma[T,],phi[T,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.66666667,  1.66666667,  1.66666667]),\n",
       " array([[ 0.33333333,  0.33333333,  0.33333333],\n",
       "        [ 0.33333333,  0.33333333,  0.33333333],\n",
       "        [ 0.33333333,  0.33333333,  0.33333333],\n",
       "        [ 0.33333333,  0.33333333,  0.33333333]]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test function\n",
    "N=4\n",
    "T=5\n",
    "alpha = np.array([1/3,1/3,1/3])\n",
    "beta = np.array([[1/2,1/2,1/2,1/2,1/2,1/2], [1/2,1/2,1/2,1/2,1/2,1/2], [1/2,1/2,1/2,1/2,1/2,1/2]])\n",
    "w=0\n",
    "a = np.stack((beta,beta))\n",
    "PhiGamma(alpha, beta, w, N, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background Introduction\n",
    "### A. Question interested\n",
    "Due to the Internet, information retrieval has exploded. For a range of reasons in the modern world, people want to compare and contrast two documents for structure, text, words, topics. The goal is to retrieve information from a large text corpus. “Thus each word is generated from a single topic, and different words in one document can be generated by different topics. Each document is represented as a list of mixing proportions for these mixture components and thereby reduced to a probability distribution on a fixed set of topics. This distribution is the ‘reduced description’ associated with the document”. \n",
    "### B. Basic thoughts and procedure\n",
    "A significant step forward in this regard was made by modeling each word in a document as a sample from a mixture model, where the mixture components are random variables that can be viewed as representations of “topics”. The solution to the goal is the Latent Dirichlet Allocation (LDA) model, a three-level probabilistic model in natural language processing for comparing and contrasting collections of discrete data, and providing a short description of the statistical relationships in those large discrete data sets. In LDA, each document is composed of various topics with Dirichlet prior distribution. Each topic has a probability of generating its corresponding word and those words that do not belong to any topic have an even probability of being placed into each category.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Algorithm \n",
    "### A. Models\n",
    "#### (1). Notation\n",
    "Formally, we define the following terms:\n",
    "- A _word_ is the basic unit of discrete data, defined to be an item from a vocabulary indexed by {1, . . . , V }. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the _v_th word in the vocabulary is represented by a V -vector w such that $w^v = 1$ and $w^u = 0$ for $u\\neq v$.\n",
    "- _A _document_ is a sequence of N words denoted by $\\textbf{w} = (w_1,w_2,... ,w_N)$, where $w_n$ is the _n_ th word in the sequence.\n",
    "- A _corpus_ is a collection of M documents denoted by $D = \\{\\textbf{w}_1,\\textbf{w}_2,... ,\\textbf{w}_M \\}$.\n",
    "\n",
    "#### (2). Latent Dirichlet allocation\n",
    "LDA assumes the following generative process for each document $\\textbf{w}$ in a corpus D:\n",
    "1. Choose N $\\sim$ Poisson(ξ).\n",
    "2. Chooseθ $\\sim$ Dir(α).\n",
    "3. For each of the N words $w_n$:\n",
    " - Choose a topic $z_n$ $\\sim$ Multinomial(θ).\n",
    " - Choose a word $w_n$ from p($w_n$ |$z_n$,β), a multinomial probability conditioned on the topic $z_n$.\n",
    "\n",
    "### B. Techniques for inference\n",
    "#### (1). Variational inference\n",
    "\n",
    "#### (2). Parameter estimation\n",
    "The derivation of the variational EM algorithm for LDA yields the following iterative algorithm:\n",
    "  1. (E-step) For each document, find the optimizing values of the variational parameters {$γ^*_d , φ^*_d : d \\in D$}. This is done as described in the previous section.\n",
    "  2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters α and β. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n",
    "\n",
    "## 3. Application\n",
    "### A. Simulated data\n",
    "### B. Real dataset\n",
    "## 4. Optimization and Analysis\n",
    "## 5. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
