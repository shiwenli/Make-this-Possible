{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL REPORT ---- Yulin Lei & Wenli Shi\n",
    "### Chosen paper: Latent Dirichlet Allocation by David M. Blei, Andrew Y. Ng and Michael I. Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background Introduction\n",
    "\n",
    "### A. Question Interested\n",
    "\n",
    "Due to the explosion of data and documents, the algorithm for modeling text corpora is urgent to be created because of the difficulty for managing documents. Before this paper, several models have be proposed by statistician and computer scientist, like information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999), latent semantic indexing (LSI) (Deerwester et al., 1990) and probabilistic LSI (pLSI) model (Hofmann 1999). From the current paper, the authors construct a Bayesian hierarchical model to describe the process of generating the documents in a corpora called Latent Dirichlet Allocation model (LDA). \n",
    "\n",
    "### B. Basic Assumptions, Thoughts and Procedure\n",
    "\n",
    "As those previous papers, this LDA model also treats the corpora as a three-level system that corpora contains documents contains bunch of words and considers words as the basic unit. However, this LDA model builds up a generating process for the words as likelihood of the Bayesian hierarchical model and assigned some priors for this process. This LDA model is constructed based on the assumption of exchangeability. Also, because of this assumption, this model ignores the relation between the adjacent words and thereby could classify them into different topics. An improvement for this is considering adding Markov Property onto the likelihood to modify the assumption of exchangeability. \n",
    "\n",
    "One feature of this model is that it adds a latent variables as \"topics\" in the model. Therefore, the process of generating words is based on the topics. While each word is associated with one topic, one document is a mixture of topics by the words in that document. To infer the interested parameteres in the model, the authors adopts variation inference and EM algorithm because of the difficulty of inference directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm \n",
    "\n",
    "### A. Models\n",
    "\n",
    "#### (1). Notation\n",
    "\n",
    "To state the detailed algorithm formally, the authors define the following terms:\n",
    "- A _word_ is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1, \\cdots , V \\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the _v_th word in the vocabulary is represented by a V-vector $w$ such that $w^v = 1$ and $w^u = 0$ for $u\\neq v$.\n",
    "- A _document_ is a sequence of $N$ words denoted by $\\textbf{w} = (w_1,w_2,... ,w_N)$, where $w_n$ is the _n_ th word in the sequence.\n",
    "- A _corpus_ is a collection of $M$ documents denoted by $D = \\{\\textbf{w}_1,\\textbf{w}_2,... ,\\textbf{w}_M \\}$.\n",
    "\n",
    "#### (2). Latent Dirichlet Allocation\n",
    "\n",
    "LDA assumes the following priors and generating process for each document $\\textbf{w}$ in a corpus D:\n",
    "1. Choose $N \\sim$ Poisson($\\xi$).\n",
    "2. Choose $\\theta \\sim$ Dir($\\alpha$).\n",
    "3. For each of the $N$ words $w_n$:\n",
    " - Choose a topic $z_n$ $\\sim$ Multinomial($\\theta$).\n",
    " - Choose a word $w_n$ from $p(w_n |z_n,\\beta)$, a multinomial probability conditioned on the topic $z_n$.\n",
    "where $N$ is the number of words and because it is independent of $\\theta$ and $\\textbf{z}$'s, its randomness could be ignored. $\\theta$, representing the mixture of topics of a word, is the parameters for the distribution of $z_n$ with dimentionality $k$ which is assumed to be known and fixed. Utilizing the conjugacy of multinomial distribution and Dirichlet distribution, the inference process would be simpler. $\\beta$ is a $k \\times V$ matrix with $\\beta_{ij} = p(w^j = 1 | z^i = 1)$.\n",
    "\n",
    "By the definition and terminology, the likelihood of a document after integrating out the topics and mixture of topics is \n",
    "\n",
    "$$p(\\textbf{w}|\\alpha,\\beta) = \\int p(\\theta|\\alpha) (\\prod_{n=1}^N \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n,\\beta)) d\\theta$$\n",
    "\n",
    "By multiplying over the corpus, the probability of a corpus is \n",
    "\n",
    "$$p(D|\\alpha,\\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) (\\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta)) d\\theta_d$$\n",
    "\n",
    "From the expression, we notice the summation within the multiplication, which causes the difficulty of inference directly. Therefore, the authors provide a method to infer the critical parameters by convexity-based variational inference and EM algorithm.\n",
    "\n",
    "### B. Techniques for Inference\n",
    "\n",
    "#### (1). Variational Inference\n",
    "\n",
    "As the process suggested, $\\beta$ and $z$ generating $w$ and $\\theta$ generating $z$, the coupling between $\\theta$ and $\\beta$ could complicate the inference procedure. To address this problem, the authors modified the model and applied the variational inference to free $\\beta$ and $\\theta$ as $\\gamma$ and $\\phi_n$. This modification leads the model as \n",
    "\n",
    "$$q(\\theta,\\textbf{z}|\\gamma,\\phi) = q(\\theta|\\gamma) \\prod_{n=1}^N q(z_n|\\phi_n)$$\n",
    "\n",
    "To determine the values of the variational parameters $\\gamma$ and $\\phi$, the authors decide to take the $(\\gamma^*, \\phi^*)$ that minimize the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior distribution $p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta)$. The iterative update equations are\n",
    "\n",
    "$$\\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{E_q[\\log(\\theta_i)|\\gamma]\\}$$\n",
    "\n",
    "$$\\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$E_q[\\log(\\theta_i)|\\gamma] = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j)$$\n",
    "\n",
    "#### (2). EM Algorithm\n",
    "\n",
    "At first, the intractable log likelihood for $\\alpha$ and $\\beta$ of the data is \n",
    "\n",
    "$$\\ell(\\alpha,\\beta) = \\sum_{d=1}^M \\log p(\\textbf{w}_d|\\alpha,\\beta)$$\n",
    "\n",
    "By variational inference, the authors provide an alternative variational EM algorithm for the approximate empirical Bayes estimates for LDA model as finding the a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$. The derivation of the variational EM algorithm for LDA yields the following iterative algorithm:\n",
    "  1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma^*_d , \\phi^*_d : d \\in D\\}$. This is done as described in the previous section.\n",
    "  2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n",
    "  3. Repeat all these two steps until the lower bound on the log likelihood converges.\n",
    "\n",
    "By this procedure, the estimate of $\\beta$ is\n",
    "\n",
    "$$\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^{N_d} \\phi^*_{dni} w^j_{dn}$$\n",
    "\n",
    "For the M-step of $\\alpha$, the authors implement using a Newton-Raphson method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implementation on Real Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random\n",
    "import numba\n",
    "from numba import jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f= open('voc', 'r')\n",
    "voc = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_vocabulary(vocab):\n",
    "    \"\"\"Parse the vocabulary set into two dictionaries, word with its index and index with its\n",
    "word. When input is null, output should be null.\"\"\"\n",
    "    type_to_index = {}\n",
    "    index_to_type = {}\n",
    "    for word in set(vocab):\n",
    "        index_to_type[len(index_to_type)] = word\n",
    "        type_to_index[word] = len(type_to_index)        \n",
    "    return (type_to_index, index_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# true data\n",
    "type_to_index = parse_vocabulary(voc)[0]\n",
    "index_to_type = parse_vocabulary(voc)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ff = open('train', 'r')\n",
    "train = ff.read().strip().split(\"\\n\")\n",
    "fff = open('test', 'r')\n",
    "test = fff.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_data(corpus):\n",
    "    \"\"\"Parse the corpus into a list of documents. Find out the index of each word in each document and count its number of appearance in each document. When input is null, output should be null.\"\"\"\n",
    "    word_ids = []\n",
    "    word_cts = []     \n",
    "    for document_line in corpus:\n",
    "        document_word_dict = {}\n",
    "        for token in document_line.split():\n",
    "            if token not in type_to_index:\n",
    "                continue                \n",
    "            type_id = type_to_index[token]\n",
    "            if type_id not in document_word_dict:\n",
    "                document_word_dict[type_id] = 0\n",
    "            document_word_dict[type_id] += 1\n",
    "\n",
    "        word_ids.append(np.array(list(document_word_dict.keys())))\n",
    "        word_cts.append(np.array(list(document_word_dict.values()))[np.newaxis, :])\n",
    "\n",
    "    return (word_ids, word_cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dirichlet_expectation(dirichlet_parameter):\n",
    "    \"\"\"Calculate the expectation of dirichlet parameter. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(dirichlet_parameter).size:\n",
    "        return (\"The dirichlet_parameter is null.\")\n",
    "    if (len(dirichlet_parameter.shape) == 1):\n",
    "        return (sp.special.psi(dirichlet_parameter)-sp.special.psi(np.sum(dirichlet_parameter)))\n",
    "    return (sp.special.psi(dirichlet_parameter) - sp.special.psi(np.sum(dirichlet_parameter, 1))[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests of function parse_vocabulary, parse_data, and compute_dirichlet_expectation, for both common and edge cases. Test results are consistent with the expectations which are described in docstrings.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'govern': 4,\n",
       "  'million': 8,\n",
       "  'new': 1,\n",
       "  'peopl': 7,\n",
       "  'percent': 3,\n",
       "  'presid': 6,\n",
       "  'report': 2,\n",
       "  'state': 5,\n",
       "  'year': 0},\n",
       " {0: 'year',\n",
       "  1: 'new',\n",
       "  2: 'report',\n",
       "  3: 'percent',\n",
       "  4: 'govern',\n",
       "  5: 'state',\n",
       "  6: 'presid',\n",
       "  7: 'peopl',\n",
       "  8: 'million'})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_test = [\"year\",\"state\",\"new\",\"percent\",\"peopl\",\"report\",\"million\",\"govern\",\"presid\"]\n",
    "parse_vocabulary(voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_test = []\n",
    "parse_vocabulary(voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([5569, 2372, 6262, 1286, 4039, 3042, 4742,  141,  656, 1427, 4438,\n",
       "         3014, 5404,  416, 4258, 1253,  551, 6058, 6514, 4790, 5112, 4089]),\n",
       "  array([5569, 1604,  709, 6793, 4749, 2388, 6486, 4089, 2008, 1115, 6306,\n",
       "         5987, 1253,  296, 5293, 2545, 4786, 3829, 3637, 3065, 2299, 5694,\n",
       "         1983]),\n",
       "  array([5569, 4039, 2504, 1866,  845, 4942, 1176, 2019, 4089, 2008, 3176,\n",
       "         1184, 6306, 5987, 4069, 4689, 5544, 2927, 2545,  882,  245, 6262,\n",
       "         3065, 4865, 5694])],\n",
       " [array([[1, 2, 3, 1, 7, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2]]),\n",
       "  array([[1, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 3, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1]]),\n",
       "  array([[1, 9, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1]])])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = [\"mauric adult adult peopl year h last year resolv polic polic polic polic polic polic polic motiv appear cri christian christian three troubl pride farley farley friday lock bureau gun gun gun church marino marino\",\"work posit reach reach peopl year thought million million million presid govern year year first percent state appear abl time billion told resid complet day develop made made made made wednesday wednesday\",\"work offer shoot thought million far year year polic polic polic polic polic polic polic polic polic cash appear abl sophist told told woman woman arrest retain found day monday citi administr prepar fame art gun\"]\n",
    "parse_data(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = []\n",
    "parse_data(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.09684835, -9.55469276, -9.38363974, ..., -9.24216926,\n",
       "        -9.35722427, -9.63080596],\n",
       "       [-9.34649163, -9.30944856, -9.50675363, ..., -9.39547571,\n",
       "        -9.40449805, -9.57647018],\n",
       "       [-9.47861401, -9.74083413, -9.25161975, ..., -9.41531582,\n",
       "        -9.48331717, -9.50022375],\n",
       "       ..., \n",
       "       [-9.18447372, -9.42226028, -9.27308997, ..., -9.21091217,\n",
       "        -9.10052444, -9.49658774],\n",
       "       [-9.2591553 , -9.46987322, -8.98963486, ..., -9.43634688,\n",
       "        -9.35462064, -9.37907071],\n",
       "       [-9.45412259, -9.43736856, -9.48812175, ..., -9.36615048,\n",
       "        -9.46575878, -9.393299  ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 100\n",
    "number_of_types = len(type_to_index)\n",
    "eta = np.random.gamma(100., 1./100., (number_of_topics,number_of_types))\n",
    "compute_dirichlet_expectation(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dirichlet_parameter is null.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_test = []\n",
    "compute_dirichlet_expectation(eta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "parsed_corpus = parse_data(train)\n",
    "test_corpus = parse_data(test)\n",
    "number_of_documents = len(parsed_corpus[0])\n",
    "\n",
    "alpha_alpha = np.zeros(number_of_topics) + 1/number_of_topics\n",
    "alpha_beta = np.zeros(number_of_types) + 1/number_of_types\n",
    "gamma = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics       \n",
    "\n",
    "hyper_parameter_optimize_interval=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_step(corpus=None,local_parameter_iteration=50,number_of_topics=number_of_topics,\n",
    "local_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"E step. When input is None, output should be document_log_likelihood, phi_sufficient_statistics, and gamma. Otherwise, it should be words_log_likelihood, gamma_values.\"\"\"\n",
    "    if corpus == None:\n",
    "        word_ids = parsed_corpus[0]\n",
    "        word_cts = parsed_corpus[1]\n",
    "    else:\n",
    "        word_ids = corpus[0]\n",
    "        word_cts = corpus[1]\n",
    "    # Initialization \n",
    "    number_of_documents = len(word_ids)\n",
    "    document_log_likelihood = 0\n",
    "    words_log_likelihood = 0\n",
    "    phi_sufficient_statistics = np.zeros((number_of_topics, number_of_types))\n",
    "    gamma_values = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics\n",
    "    E_log_eta = compute_dirichlet_expectation(eta)\n",
    "    if parsed_corpus != None:\n",
    "        E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # iterate over all documents\n",
    "    for doc_id in np.random.permutation(number_of_documents):\n",
    "        # compute the total number of words\n",
    "        total_word_count = np.sum(word_cts[doc_id])\n",
    "        # initialize gamma for this document\n",
    "        gamma_values[doc_id, :] = alpha_alpha + 1.0 * total_word_count / number_of_topics\n",
    "\n",
    "        term_ids = word_ids[doc_id]\n",
    "        term_counts = word_cts[doc_id]\n",
    "\n",
    "        # update phi and gamma until gamma converges\n",
    "        for gamma_iteration in range(local_parameter_iteration):\n",
    "            log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[doc_id, :]), (word_ids[doc_id].shape[0], 1))\n",
    "            log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis]\n",
    "            gamma_update = alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(np.repeat(term_counts, number_of_topics, axis=0).T)), axis=0))\n",
    "            mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]))\n",
    "            gamma_values[doc_id, :] = gamma_update\n",
    "            if mean_change <= local_parameter_converge_threshold:\n",
    "                break\n",
    "\n",
    "        # compute the alpha, gamma, and phi terms\n",
    "        document_log_likelihood += sp.special.gammaln(np.sum(alpha_alpha)) - np.sum(sp.special.gammaln(alpha_alpha))\n",
    "        document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]))\n",
    "        document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi))\n",
    "\n",
    "# compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step\n",
    "        words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids])      \n",
    "        phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T\n",
    "        \n",
    "    if corpus == None:\n",
    "        gamma = gamma_values\n",
    "        return (document_log_likelihood, phi_sufficient_statistics, gamma)\n",
    "    else:\n",
    "        return (words_log_likelihood, gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def m_step(phi_sufficient_statistics):\n",
    "    \"\"\"M step. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(phi_sufficient_statistics).size:\n",
    "        return (\"The input is null.\")\n",
    "    # compute the beta and the eta terms\n",
    "    topic_log_likelihood = number_of_topics * (sp.special.gammaln(np.sum(alpha_beta)) - np.sum(sp.special.gammaln(alpha_beta)))\n",
    "    topic_log_likelihood += np.sum(np.sum(sp.special.gammaln(eta), axis=1) - sp.special.gammaln(np.sum(eta, axis=1)))\n",
    "\n",
    "    eta_temp = phi_sufficient_statistics + alpha_beta\n",
    "\n",
    "    # compute the sufficient statistics for alpha and update\n",
    "    alpha_sufficient_statistics = sp.special.psi(gamma) - sp.special.psi(np.sum(gamma, axis=1)[:, np.newaxis])\n",
    "    alpha_sufficient_statistics = np.sum(alpha_sufficient_statistics, axis=0)  \n",
    "\n",
    "    return (topic_log_likelihood, alpha_sufficient_statistics, eta_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute likelihood and sufficient statistics. Also serve as a testing when input of e_step is null. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement M step to fit the model and test function m_step when input is null.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The input is null.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_log_likelihood, alpha_sufficient_statistics, eta = m_step(phi_sufficient_statistics)\n",
    "phi_sufficient_statistics_test = []\n",
    "m_step(phi_sufficient_statistics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(alpha_sufficient_statistics, hyper_parameter_iteration=100, hyper_parameter_decay_factor=0.9, hyper_parameter_maximum_decay=10,alpha=alpha_alpha, hyper_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"Optimize hyperparameter alpha. Since the function's input is the result of m_step function, it is unnecessary and difficult to test it. Just use it.\"\"\"\n",
    "    alpha_update = alpha        \n",
    "    decay = 0\n",
    "    for alpha_iteration in range(hyper_parameter_iteration):\n",
    "        alpha_gradient = number_of_documents * (sp.special.psi(np.sum(alpha)) - sp.special.psi(alpha)) + alpha_sufficient_statistics\n",
    "        alpha_hessian = -number_of_documents * sp.special.polygamma(1,alpha)\n",
    "\n",
    "        sum_g_h = np.sum(alpha_gradient / alpha_hessian)\n",
    "        sum_1_h = 1.0 / alpha_hessian\n",
    "        z = number_of_documents * sp.special.polygamma(1, np.sum(alpha))\n",
    "        c = sum_g_h / (1.0 / z + sum_1_h)\n",
    "\n",
    "        # update the alpha vector\n",
    "        while True:\n",
    "            singular_hessian = False\n",
    "            step_size = np.power(hyper_parameter_decay_factor, decay) * (alpha_gradient - c) / alpha_hessian               \n",
    "            if np.any(alpha <= step_size):\n",
    "                singular_hessian = True\n",
    "            else: alpha_update = alpha - step_size\n",
    "\n",
    "            if singular_hessian:\n",
    "                decay += 1\n",
    "                if decay > hyper_parameter_maximum_decay:\n",
    "                    break\n",
    "            else: break\n",
    "\n",
    "        # check the alpha converge criteria\n",
    "        mean_change = np.mean(abs(alpha_update - alpha))\n",
    "        alpha = alpha_update\n",
    "        if mean_change <= hyper_parameter_converge_threshold:\n",
    "            break\n",
    "\n",
    "    return (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_alpha = optimize_hyperparameters(alpha_sufficient_statistics)\n",
    "words_log_likelihood, corpus_gamma_values = e_step(corpus = test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every topic, find out the top 20 words that are most likely to appear in this topic. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>jackson</td>\n",
       "      <td>state</td>\n",
       "      <td>democrat</td>\n",
       "      <td>report</td>\n",
       "      <td>polic</td>\n",
       "      <td>soviet</td>\n",
       "      <td>japan</td>\n",
       "      <td>jackson</td>\n",
       "      <td>percent</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>support</td>\n",
       "      <td>parti</td>\n",
       "      <td>dukaki</td>\n",
       "      <td>militari</td>\n",
       "      <td>two</td>\n",
       "      <td>offici</td>\n",
       "      <td>govern</td>\n",
       "      <td>dukaki</td>\n",
       "      <td>report</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>robb</td>\n",
       "      <td>peopl</td>\n",
       "      <td>jackson</td>\n",
       "      <td>peopl</td>\n",
       "      <td>state</td>\n",
       "      <td>say</td>\n",
       "      <td>trade</td>\n",
       "      <td>parti</td>\n",
       "      <td>million</td>\n",
       "      <td>compani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campaign</td>\n",
       "      <td>say</td>\n",
       "      <td>offici</td>\n",
       "      <td>year</td>\n",
       "      <td>peopl</td>\n",
       "      <td>militari</td>\n",
       "      <td>new</td>\n",
       "      <td>vote</td>\n",
       "      <td>last</td>\n",
       "      <td>defen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>democrat</td>\n",
       "      <td>govern</td>\n",
       "      <td>day</td>\n",
       "      <td>parti</td>\n",
       "      <td>offici</td>\n",
       "      <td>reagan</td>\n",
       "      <td>offici</td>\n",
       "      <td>presid</td>\n",
       "      <td>peopl</td>\n",
       "      <td>american</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>say</td>\n",
       "      <td>presid</td>\n",
       "      <td>aid</td>\n",
       "      <td>govern</td>\n",
       "      <td>new</td>\n",
       "      <td>presid</td>\n",
       "      <td>last</td>\n",
       "      <td>bush</td>\n",
       "      <td>state</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dukaki</td>\n",
       "      <td>new</td>\n",
       "      <td>committ</td>\n",
       "      <td>soviet</td>\n",
       "      <td>charg</td>\n",
       "      <td>unit</td>\n",
       "      <td>report</td>\n",
       "      <td>democrat</td>\n",
       "      <td>compani</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>deleg</td>\n",
       "      <td>year</td>\n",
       "      <td>bush</td>\n",
       "      <td>nation</td>\n",
       "      <td>year</td>\n",
       "      <td>govern</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>dukaki</td>\n",
       "      <td>depart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>presid</td>\n",
       "      <td>polic</td>\n",
       "      <td>first</td>\n",
       "      <td>forc</td>\n",
       "      <td>told</td>\n",
       "      <td>state</td>\n",
       "      <td>presid</td>\n",
       "      <td>state</td>\n",
       "      <td>year</td>\n",
       "      <td>soviet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>convent</td>\n",
       "      <td>time</td>\n",
       "      <td>group</td>\n",
       "      <td>gorbachev</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>union</td>\n",
       "      <td>week</td>\n",
       "      <td>nation</td>\n",
       "      <td>nation</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>breast</td>\n",
       "      <td>offici</td>\n",
       "      <td>year</td>\n",
       "      <td>plan</td>\n",
       "      <td>forc</td>\n",
       "      <td>new</td>\n",
       "      <td>american</td>\n",
       "      <td>peopl</td>\n",
       "      <td>two</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>vote</td>\n",
       "      <td>million</td>\n",
       "      <td>support</td>\n",
       "      <td>state</td>\n",
       "      <td>plan</td>\n",
       "      <td>book</td>\n",
       "      <td>year</td>\n",
       "      <td>govern</td>\n",
       "      <td>say</td>\n",
       "      <td>million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>court</td>\n",
       "      <td>two</td>\n",
       "      <td>percent</td>\n",
       "      <td>day</td>\n",
       "      <td>unit</td>\n",
       "      <td>first</td>\n",
       "      <td>told</td>\n",
       "      <td>offici</td>\n",
       "      <td>govern</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nomin</td>\n",
       "      <td>student</td>\n",
       "      <td>deleg</td>\n",
       "      <td>presid</td>\n",
       "      <td>school</td>\n",
       "      <td>defen</td>\n",
       "      <td>time</td>\n",
       "      <td>support</td>\n",
       "      <td>first</td>\n",
       "      <td>presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>think</td>\n",
       "      <td>report</td>\n",
       "      <td>hou</td>\n",
       "      <td>say</td>\n",
       "      <td>man</td>\n",
       "      <td>time</td>\n",
       "      <td>peopl</td>\n",
       "      <td>new</td>\n",
       "      <td>feder</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>day</td>\n",
       "      <td>leader</td>\n",
       "      <td>peopl</td>\n",
       "      <td>elect</td>\n",
       "      <td>made</td>\n",
       "      <td>last</td>\n",
       "      <td>say</td>\n",
       "      <td>campaign</td>\n",
       "      <td>plan</td>\n",
       "      <td>think</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>candid</td>\n",
       "      <td>share</td>\n",
       "      <td>issu</td>\n",
       "      <td>rule</td>\n",
       "      <td>last</td>\n",
       "      <td>hou</td>\n",
       "      <td>investig</td>\n",
       "      <td>forc</td>\n",
       "      <td>new</td>\n",
       "      <td>sale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>walesa</td>\n",
       "      <td>forc</td>\n",
       "      <td>meet</td>\n",
       "      <td>leader</td>\n",
       "      <td>say</td>\n",
       "      <td>peopl</td>\n",
       "      <td>parti</td>\n",
       "      <td>leader</td>\n",
       "      <td>presid</td>\n",
       "      <td>offici</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>like</td>\n",
       "      <td>unit</td>\n",
       "      <td>nation</td>\n",
       "      <td>time</td>\n",
       "      <td>work</td>\n",
       "      <td>mr</td>\n",
       "      <td>work</td>\n",
       "      <td>elect</td>\n",
       "      <td>american</td>\n",
       "      <td>stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>percent</td>\n",
       "      <td>last</td>\n",
       "      <td>presidenti</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>found</td>\n",
       "      <td>tax</td>\n",
       "      <td>bush</td>\n",
       "      <td>year</td>\n",
       "      <td>union</td>\n",
       "      <td>reagan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic0   topic1      topic2     topic3   topic4    topic5    topic6  \\\n",
       "0    jackson    state    democrat     report    polic    soviet     japan   \n",
       "1    support    parti      dukaki   militari      two    offici    govern   \n",
       "2       robb    peopl     jackson      peopl    state       say     trade   \n",
       "3   campaign      say      offici       year    peopl  militari       new   \n",
       "4   democrat   govern         day      parti   offici    reagan    offici   \n",
       "5        say   presid         aid     govern      new    presid      last   \n",
       "6     dukaki      new     committ     soviet    charg      unit    report   \n",
       "7      deleg     year        bush     nation     year    govern       two   \n",
       "8     presid    polic       first       forc     told     state    presid   \n",
       "9    convent     time       group  gorbachev  tuesday     union      week   \n",
       "10    breast   offici        year       plan     forc       new  american   \n",
       "11      vote  million     support      state     plan      book      year   \n",
       "12     court      two     percent        day     unit     first      told   \n",
       "13     nomin  student       deleg     presid   school     defen      time   \n",
       "14     think   report         hou        say      man      time     peopl   \n",
       "15       day   leader       peopl      elect     made      last       say   \n",
       "16    candid    share        issu       rule     last       hou  investig   \n",
       "17    walesa     forc        meet     leader      say     peopl     parti   \n",
       "18      like     unit      nation       time     work        mr      work   \n",
       "19   percent     last  presidenti    tuesday    found       tax      bush   \n",
       "\n",
       "      topic7    topic8    topic9  \n",
       "0    jackson   percent       new  \n",
       "1     dukaki    report      year  \n",
       "2      parti   million   compani  \n",
       "3       vote      last     defen  \n",
       "4     presid     peopl  american  \n",
       "5       bush     state     peopl  \n",
       "6   democrat   compani      last  \n",
       "7        two    dukaki    depart  \n",
       "8      state      year    soviet  \n",
       "9     nation    nation    system  \n",
       "10     peopl       two       say  \n",
       "11    govern       say   million  \n",
       "12    offici    govern      time  \n",
       "13   support     first    presid  \n",
       "14       new     feder    report  \n",
       "15  campaign      plan     think  \n",
       "16      forc       new      sale  \n",
       "17    leader    presid    offici  \n",
       "18     elect  american     stock  \n",
       "19      year     union    reagan  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_log_eta = compute_dirichlet_expectation(eta)\n",
    "topic = []\n",
    "for topic_index in range(number_of_topics):\n",
    "    temp_list = []\n",
    "    beta_probability = np.exp(E_log_eta[topic_index, :] - sp.misc.logsumexp(E_log_eta[topic_index, :]))\n",
    "    i = 0\n",
    "    for type_index in reversed(np.argsort(beta_probability)):\n",
    "        i += 1\n",
    "        if i <= 20:\n",
    "            temp_list.append(index_to_type[type_index])\n",
    "        else: break\n",
    "    topic.append(temp_list)\n",
    "\n",
    "index = random.sample(range(number_of_topics),10)\n",
    "df = pd.DataFrame({\"topic0\":topic[index[9]],\"topic1\":topic[index[0]],\"topic2\":topic[index[1]],\"topic3\":topic[index[2]],\"topic4\":topic[index[3]],\"topic5\":topic[index[4]],\"topic6\":topic[index[5]],\"topic7\":topic[index[6]],\"topic8\":topic[index[7]],\"topic9\":topic[index[8]]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_dirichlet_expectation(dirichlet_parameter):\n",
    "    \"\"\"Calculate the expectation of dirichlet parameter. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(dirichlet_parameter).size:\n",
    "        return (\"The dirichlet_parameter is null.\")\n",
    "    if (len(dirichlet_parameter.shape) == 1):\n",
    "        return (sp.special.psi(dirichlet_parameter)-sp.special.psi(np.sum(dirichlet_parameter)))\n",
    "    return (sp.special.psi(dirichlet_parameter) - sp.special.psi(np.sum(dirichlet_parameter, 1))[:, np.newaxis])\n",
    "\n",
    "\n",
    "def e_step(corpus=None,local_parameter_iteration=50,number_of_topics=number_of_topics,\n",
    "local_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"E step. When input is None, output should be document_log_likelihood, phi_sufficient_statistics, and gamma. Otherwise, it should be words_log_likelihood, gamma_values.\"\"\"\n",
    "    if corpus == None:\n",
    "        word_ids = parsed_corpus[0]\n",
    "        word_cts = parsed_corpus[1]\n",
    "    else:\n",
    "        word_ids = corpus[0]\n",
    "        word_cts = corpus[1]\n",
    "    # Initialization \n",
    "    number_of_documents = len(word_ids)\n",
    "    document_log_likelihood = 0\n",
    "    words_log_likelihood = 0\n",
    "    phi_sufficient_statistics = np.zeros((number_of_topics, number_of_types))\n",
    "    gamma_values = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics\n",
    "    E_log_eta = compute_dirichlet_expectation(eta)\n",
    "    if parsed_corpus != None:\n",
    "        E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # iterate over all documents\n",
    "    for doc_id in np.random.permutation(number_of_documents):\n",
    "        # compute the total number of words\n",
    "        total_word_count = np.sum(word_cts[doc_id])\n",
    "        # initialize gamma for this document\n",
    "        gamma_values[doc_id, :] = alpha_alpha + 1.0 * total_word_count / number_of_topics\n",
    "\n",
    "        term_ids = word_ids[doc_id]\n",
    "        term_counts = word_cts[doc_id]\n",
    "\n",
    "        # update phi and gamma until gamma converges\n",
    "        for gamma_iteration in range(local_parameter_iteration):\n",
    "            log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[doc_id, :]), (word_ids[doc_id].shape[0], 1))\n",
    "            log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis]\n",
    "            gamma_update = alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(np.repeat(term_counts, number_of_topics, axis=0).T)), axis=0))\n",
    "            mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]))\n",
    "            gamma_values[doc_id, :] = gamma_update\n",
    "            if mean_change <= local_parameter_converge_threshold:\n",
    "                break\n",
    "\n",
    "        # compute the alpha, gamma, and phi terms\n",
    "        document_log_likelihood += sp.special.gammaln(np.sum(alpha_alpha)) - np.sum(sp.special.gammaln(alpha_alpha))\n",
    "        document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]))\n",
    "        document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi))\n",
    "\n",
    "# compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step\n",
    "        words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids])      \n",
    "        phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T\n",
    "        \n",
    "    if corpus == None:\n",
    "        gamma = gamma_values\n",
    "        return (document_log_likelihood, phi_sufficient_statistics, gamma)\n",
    "    else:\n",
    "        return (words_log_likelihood, gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.7 s, sys: 548 ms, total: 54.3 s\n",
      "Wall time: 54.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dirichlet_expectation(dirichlet_parameter):\n",
    "    \"\"\"Calculate the expectation of dirichlet parameter. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(dirichlet_parameter).size:\n",
    "        return (\"The dirichlet_parameter is null.\")\n",
    "    if (len(dirichlet_parameter.shape) == 1):\n",
    "        return (sp.special.psi(dirichlet_parameter)-sp.special.psi(np.sum(dirichlet_parameter)))\n",
    "    return (sp.special.psi(dirichlet_parameter) - sp.special.psi(np.sum(dirichlet_parameter, 1))[:, np.newaxis])\n",
    "\n",
    "\n",
    "def e_step(corpus=None,local_parameter_iteration=50,number_of_topics=number_of_topics,\n",
    "local_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"E step. When input is None, output should be document_log_likelihood, phi_sufficient_statistics, and gamma. Otherwise, it should be words_log_likelihood, gamma_values.\"\"\"\n",
    "    if corpus == None:\n",
    "        word_ids = parsed_corpus[0]\n",
    "        word_cts = parsed_corpus[1]\n",
    "    else:\n",
    "        word_ids = corpus[0]\n",
    "        word_cts = corpus[1]\n",
    "    # Initialization \n",
    "    number_of_documents = len(word_ids)\n",
    "    document_log_likelihood = 0\n",
    "    words_log_likelihood = 0\n",
    "    phi_sufficient_statistics = np.zeros((number_of_topics, number_of_types))\n",
    "    gamma_values = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics\n",
    "    E_log_eta = compute_dirichlet_expectation(eta)\n",
    "    if parsed_corpus != None:\n",
    "        E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # iterate over all documents\n",
    "    for doc_id in np.random.permutation(number_of_documents):\n",
    "        # compute the total number of words\n",
    "        total_word_count = np.sum(word_cts[doc_id])\n",
    "        # initialize gamma for this document\n",
    "        gamma_values[doc_id, :] = alpha_alpha + 1.0 * total_word_count / number_of_topics\n",
    "\n",
    "        term_ids = word_ids[doc_id]\n",
    "        term_counts = word_cts[doc_id]\n",
    "\n",
    "        # update phi and gamma until gamma converges\n",
    "        for gamma_iteration in range(local_parameter_iteration):\n",
    "            log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[doc_id, :]), (word_ids[doc_id].shape[0], 1))\n",
    "            log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis]\n",
    "            gamma_update = alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(np.repeat(term_counts, number_of_topics, axis=0).T)), axis=0))\n",
    "            mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]))\n",
    "            gamma_values[doc_id, :] = gamma_update\n",
    "            if mean_change <= local_parameter_converge_threshold:\n",
    "                break\n",
    "\n",
    "        # compute the alpha, gamma, and phi terms\n",
    "        document_log_likelihood += sp.special.gammaln(np.sum(alpha_alpha)) - np.sum(sp.special.gammaln(alpha_alpha))\n",
    "        document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]))\n",
    "        document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi))\n",
    "\n",
    "# compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step\n",
    "        words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids])      \n",
    "        phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T\n",
    "        \n",
    "    if corpus == None:\n",
    "        gamma = gamma_values\n",
    "        return (document_log_likelihood, phi_sufficient_statistics, gamma)\n",
    "    else:\n",
    "        return (words_log_likelihood, gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.8 s, sys: 437 ms, total: 53.2 s\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the number of topics and compare the results.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The reason why the following code is not wrapped into a function is that many global variables are used and there will be conflicts if putting it in a function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>percent</td>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>percent</td>\n",
       "      <td>year</td>\n",
       "      <td>two</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>state</td>\n",
       "      <td>percent</td>\n",
       "      <td>new</td>\n",
       "      <td>govern</td>\n",
       "      <td>new</td>\n",
       "      <td>million</td>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>new</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nation</td>\n",
       "      <td>say</td>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>soviet</td>\n",
       "      <td>percent</td>\n",
       "      <td>report</td>\n",
       "      <td>last</td>\n",
       "      <td>year</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presid</td>\n",
       "      <td>state</td>\n",
       "      <td>report</td>\n",
       "      <td>compani</td>\n",
       "      <td>million</td>\n",
       "      <td>billion</td>\n",
       "      <td>new</td>\n",
       "      <td>two</td>\n",
       "      <td>presid</td>\n",
       "      <td>presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>peopl</td>\n",
       "      <td>million</td>\n",
       "      <td>peopl</td>\n",
       "      <td>offici</td>\n",
       "      <td>percent</td>\n",
       "      <td>new</td>\n",
       "      <td>say</td>\n",
       "      <td>offici</td>\n",
       "      <td>state</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>say</td>\n",
       "      <td>govern</td>\n",
       "      <td>million</td>\n",
       "      <td>presid</td>\n",
       "      <td>presid</td>\n",
       "      <td>peopl</td>\n",
       "      <td>day</td>\n",
       "      <td>say</td>\n",
       "      <td>peopl</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>year</td>\n",
       "      <td>new</td>\n",
       "      <td>govern</td>\n",
       "      <td>two</td>\n",
       "      <td>polic</td>\n",
       "      <td>last</td>\n",
       "      <td>time</td>\n",
       "      <td>polic</td>\n",
       "      <td>nation</td>\n",
       "      <td>soviet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unit</td>\n",
       "      <td>report</td>\n",
       "      <td>cent</td>\n",
       "      <td>last</td>\n",
       "      <td>two</td>\n",
       "      <td>nation</td>\n",
       "      <td>soviet</td>\n",
       "      <td>nation</td>\n",
       "      <td>govern</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>new</td>\n",
       "      <td>hou</td>\n",
       "      <td>polic</td>\n",
       "      <td>peopl</td>\n",
       "      <td>state</td>\n",
       "      <td>state</td>\n",
       "      <td>govern</td>\n",
       "      <td>report</td>\n",
       "      <td>polic</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>report</td>\n",
       "      <td>peopl</td>\n",
       "      <td>nation</td>\n",
       "      <td>unit</td>\n",
       "      <td>report</td>\n",
       "      <td>month</td>\n",
       "      <td>offici</td>\n",
       "      <td>govern</td>\n",
       "      <td>offici</td>\n",
       "      <td>offici</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>day</td>\n",
       "      <td>presid</td>\n",
       "      <td>day</td>\n",
       "      <td>report</td>\n",
       "      <td>unit</td>\n",
       "      <td>report</td>\n",
       "      <td>peopl</td>\n",
       "      <td>percent</td>\n",
       "      <td>report</td>\n",
       "      <td>govern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bush</td>\n",
       "      <td>two</td>\n",
       "      <td>say</td>\n",
       "      <td>nation</td>\n",
       "      <td>time</td>\n",
       "      <td>market</td>\n",
       "      <td>first</td>\n",
       "      <td>time</td>\n",
       "      <td>say</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>govern</td>\n",
       "      <td>vote</td>\n",
       "      <td>york</td>\n",
       "      <td>plan</td>\n",
       "      <td>work</td>\n",
       "      <td>presid</td>\n",
       "      <td>last</td>\n",
       "      <td>offic</td>\n",
       "      <td>price</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>two</td>\n",
       "      <td>feder</td>\n",
       "      <td>time</td>\n",
       "      <td>parti</td>\n",
       "      <td>say</td>\n",
       "      <td>stock</td>\n",
       "      <td>nation</td>\n",
       "      <td>american</td>\n",
       "      <td>forc</td>\n",
       "      <td>citi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>soviet</td>\n",
       "      <td>today</td>\n",
       "      <td>last</td>\n",
       "      <td>new</td>\n",
       "      <td>offici</td>\n",
       "      <td>offici</td>\n",
       "      <td>state</td>\n",
       "      <td>presid</td>\n",
       "      <td>trade</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>offic</td>\n",
       "      <td>court</td>\n",
       "      <td>offici</td>\n",
       "      <td>group</td>\n",
       "      <td>peopl</td>\n",
       "      <td>compani</td>\n",
       "      <td>hou</td>\n",
       "      <td>peopl</td>\n",
       "      <td>million</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>go</td>\n",
       "      <td>last</td>\n",
       "      <td>market</td>\n",
       "      <td>time</td>\n",
       "      <td>nation</td>\n",
       "      <td>first</td>\n",
       "      <td>two</td>\n",
       "      <td>new</td>\n",
       "      <td>west</td>\n",
       "      <td>thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>forc</td>\n",
       "      <td>congress</td>\n",
       "      <td>unit</td>\n",
       "      <td>offic</td>\n",
       "      <td>compani</td>\n",
       "      <td>feder</td>\n",
       "      <td>presid</td>\n",
       "      <td>first</td>\n",
       "      <td>american</td>\n",
       "      <td>million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>time</td>\n",
       "      <td>told</td>\n",
       "      <td>today</td>\n",
       "      <td>three</td>\n",
       "      <td>three</td>\n",
       "      <td>drug</td>\n",
       "      <td>week</td>\n",
       "      <td>bush</td>\n",
       "      <td>last</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>million</td>\n",
       "      <td>member</td>\n",
       "      <td>share</td>\n",
       "      <td>work</td>\n",
       "      <td>stock</td>\n",
       "      <td>rate</td>\n",
       "      <td>plan</td>\n",
       "      <td>million</td>\n",
       "      <td>offic</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic0    topic1   topic2   topic3   topic4   topic5   topic6    topic7  \\\n",
       "0   percent      year    state     year     year     year  percent      year   \n",
       "1     state   percent      new   govern      new  million     year     state   \n",
       "2    nation       say     year    state   soviet  percent   report      last   \n",
       "3    presid     state   report  compani  million  billion      new       two   \n",
       "4     peopl   million    peopl   offici  percent      new      say    offici   \n",
       "5       say    govern  million   presid   presid    peopl      day       say   \n",
       "6      year       new   govern      two    polic     last     time     polic   \n",
       "7      unit    report     cent     last      two   nation   soviet    nation   \n",
       "8       new       hou    polic    peopl    state    state   govern    report   \n",
       "9    report     peopl   nation     unit   report    month   offici    govern   \n",
       "10      day    presid      day   report     unit   report    peopl   percent   \n",
       "11     bush       two      say   nation     time   market    first      time   \n",
       "12   govern      vote     york     plan     work   presid     last     offic   \n",
       "13      two     feder     time    parti      say    stock   nation  american   \n",
       "14   soviet     today     last      new   offici   offici    state    presid   \n",
       "15    offic     court   offici    group    peopl  compani      hou     peopl   \n",
       "16       go      last   market     time   nation    first      two       new   \n",
       "17     forc  congress     unit    offic  compani    feder   presid     first   \n",
       "18     time      told    today    three    three     drug     week      bush   \n",
       "19  million    member    share     work    stock     rate     plan   million   \n",
       "\n",
       "      topic8    topic9  \n",
       "0        two     peopl  \n",
       "1        new      year  \n",
       "2       year       new  \n",
       "3     presid    presid  \n",
       "4      state     state  \n",
       "5      peopl      bush  \n",
       "6     nation    soviet  \n",
       "7     govern       say  \n",
       "8      polic    report  \n",
       "9     offici    offici  \n",
       "10    report    govern  \n",
       "11       say      last  \n",
       "12     price       two  \n",
       "13      forc      citi  \n",
       "14     trade      time  \n",
       "15   million        go  \n",
       "16      west  thursday  \n",
       "17  american   million  \n",
       "18      last      call  \n",
       "19     offic    member  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 10\n",
    "alpha_alpha = np.zeros(number_of_topics) + 1/number_of_topics\n",
    "alpha_beta = np.zeros(number_of_types) + 1/number_of_types\n",
    "eta = np.random.gamma(100., 1./100., (number_of_topics,number_of_types))\n",
    "gamma = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics \n",
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step(number_of_topics=number_of_topics)\n",
    "topic_log_likelihood, alpha_sufficient_statistics, eta = m_step(phi_sufficient_statistics)\n",
    "alpha_alpha = optimize_hyperparameters(alpha_sufficient_statistics,alpha=alpha_alpha)\n",
    "words_log_likelihood, corpus_gamma_values = e_step(corpus = test_corpus,\n",
    "number_of_topics=number_of_topics)\n",
    "E_log_eta = compute_dirichlet_expectation(eta)\n",
    "topic = []\n",
    "for topic_index in range(number_of_topics):\n",
    "    temp_list = []\n",
    "    beta_probability = np.exp(E_log_eta[topic_index, :] - sp.misc.logsumexp(E_log_eta[topic_index, :]))\n",
    "    i = 0\n",
    "    for type_index in reversed(np.argsort(beta_probability)):\n",
    "        i += 1\n",
    "        if i <= 20:\n",
    "            temp_list.append(index_to_type[type_index])\n",
    "        else: break\n",
    "    topic.append(temp_list)\n",
    "\n",
    "index = random.sample(range(number_of_topics),10)\n",
    "df = pd.DataFrame({\"topic0\":topic[index[9]],\"topic1\":topic[index[0]],\"topic2\":topic[index[1]],\"topic3\":topic[index[2]],\"topic4\":topic[index[3]],\"topic5\":topic[index[4]],\"topic6\":topic[index[5]],\"topic7\":topic[index[6]],\"topic8\":topic[index[7]],\"topic9\":topic[index[8]]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we just have 10 topics, we can see that words in different topics are similar. Because test corpus is not that large and there are not so many key words, the corpus is not fully divided into different topics and different topics will overlap with each other. So when the number of topics is small, those key words will appear repeatedly and the distinctions of topics are not obvious.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
