{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL REPORT ---- Yulin Lei & Wenli Shi\n",
    "### Chosen paper: Latent Dirichlet Allocation by David M. Blei, Andrew Y. Ng and Michael I. Jordan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background Introduction\n",
    "\n",
    "### A. Question Interested\n",
    "\n",
    "Due to the explosion of data and documents, the algorithm for modeling text corpora is urgent to be created because of the difficulty for managing documents. Before this paper, several models have be proposed by statistician and computer scientist, like information retrieval (IR) (Baeza-Yates and Ribeiro-Neto, 1999), latent semantic indexing (LSI) (Deerwester et al., 1990) and probabilistic LSI (pLSI) model (Hofmann 1999). From the current paper, the authors construct a Bayesian hierarchical model to describe the process of generating the documents in a corpora called Latent Dirichlet Allocation model (LDA). \n",
    "\n",
    "### B. Basic Assumptions, Thoughts and Procedure\n",
    "\n",
    "As those previous papers, this LDA model also treats the corpora as a three-level system that corpora contains documents contains bunch of words and considers words as the basic unit. However, this LDA model builds up a generating process for the words as likelihood of the Bayesian hierarchical model and assigned some priors for this process. This LDA model is constructed based on the assumption of exchangeability. Also, because of this assumption, this model ignores the relation between the adjacent words and thereby could classify them into different topics. An improvement for this is considering adding Markov Property onto the likelihood to modify the assumption of exchangeability. \n",
    "\n",
    "One feature of this model is that it adds a latent variables as \"topics\" in the model. Therefore, the process of generating words is based on the topics. While each word is associated with one topic, one document is a mixture of topics by the words in that document. To infer the interested parameteres in the model, the authors adopts variation inference and EM algorithm because of the difficulty of inference directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm \n",
    "\n",
    "### A. Models\n",
    "\n",
    "#### (1). Notation\n",
    "\n",
    "To state the detailed algorithm formally, the authors define the following terms:\n",
    "- A _word_ is the basic unit of discrete data, defined to be an item from a vocabulary indexed by $\\{1, \\cdots , V \\}$. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero. Thus, using superscripts to denote components, the _v_th word in the vocabulary is represented by a V-vector $w$ such that $w^v = 1$ and $w^u = 0$ for $u\\neq v$.\n",
    "- A _document_ is a sequence of $N$ words denoted by $\\textbf{w} = (w_1,w_2,... ,w_N)$, where $w_n$ is the _n_ th word in the sequence.\n",
    "- A _corpus_ is a collection of $M$ documents denoted by $D = \\{\\textbf{w}_1,\\textbf{w}_2,... ,\\textbf{w}_M \\}$.\n",
    "\n",
    "#### (2). Latent Dirichlet Allocation\n",
    "\n",
    "LDA assumes the following priors and generating process for each document $\\textbf{w}$ in a corpus D:\n",
    "1. Choose $N \\sim$ Poisson($\\xi$).\n",
    "2. Choose $\\theta \\sim$ Dir($\\alpha$).\n",
    "3. For each of the $N$ words $w_n$:\n",
    " - Choose a topic $z_n$ $\\sim$ Multinomial($\\theta$).\n",
    " - Choose a word $w_n$ from $p(w_n |z_n,\\beta)$, a multinomial probability conditioned on the topic $z_n$.\n",
    "where $N$ is the number of words and because it is independent of $\\theta$ and $\\textbf{z}$'s, its randomness could be ignored. $\\theta$, representing the mixture of topics of a word, is the parameters for the distribution of $z_n$ with dimentionality $k$ which is assumed to be known and fixed. Utilizing the conjugacy of multinomial distribution and Dirichlet distribution, the inference process would be simpler. $\\beta$ is a $k \\times V$ matrix with $\\beta_{ij} = p(w^j = 1 | z^i = 1)$.\n",
    "\n",
    "By the definition and terminology, the likelihood of a document after integrating out the topics and mixture of topics is \n",
    "\n",
    "$$p(\\textbf{w}|\\alpha,\\beta) = \\int p(\\theta|\\alpha) (\\prod_{n=1}^N \\sum_{z_n} p(z_n|\\theta) p(w_n|z_n,\\beta)) d\\theta$$\n",
    "\n",
    "By multiplying over the corpus, the probability of a corpus is \n",
    "\n",
    "$$p(D|\\alpha,\\beta) = \\prod_{d=1}^M \\int p(\\theta_d|\\alpha) (\\prod_{n=1}^{N_d} \\sum_{z_{dn}} p(z_{dn}|\\theta_d) p(w_{dn}|z_{dn},\\beta)) d\\theta_d$$\n",
    "\n",
    "From the expression, we notice the summation within the multiplication, which causes the difficulty of inference directly. Therefore, the authors provide a method to infer the critical parameters by convexity-based variational inference and EM algorithm.\n",
    "\n",
    "### B. Techniques for Inference\n",
    "\n",
    "#### (1). Variational Inference\n",
    "\n",
    "As the process suggested, $\\beta$ and $z$ generating $w$ and $\\theta$ generating $z$, the coupling between $\\theta$ and $\\beta$ could complicate the inference procedure. To address this problem, the authors modified the model and applied the variational inference to free $\\beta$ and $\\theta$ as $\\gamma$ and $\\phi_n$. This modification leads the model as \n",
    "\n",
    "$$q(\\theta,\\textbf{z}|\\gamma,\\phi) = q(\\theta|\\gamma) \\prod_{n=1}^N q(z_n|\\phi_n)$$\n",
    "\n",
    "To determine the values of the variational parameters $\\gamma$ and $\\phi$, the authors decide to take the $(\\gamma^*, \\phi^*)$ that minimize the Kullback-Leibler (KL) divergence between the variational distribution and the true posterior distribution $p(\\theta,\\textbf{z}|\\textbf{w},\\alpha,\\beta)$. The iterative update equations are\n",
    "\n",
    "$$\\phi_{ni} \\propto \\beta_{iw_n} \\exp\\{E_q[\\log(\\theta_i)|\\gamma]\\}$$\n",
    "\n",
    "$$\\gamma_i = \\alpha_i + \\sum_{n=1}^N \\phi_{ni}$$\n",
    "\n",
    "where \n",
    "\n",
    "$$E_q[\\log(\\theta_i)|\\gamma] = \\Psi(\\gamma_i) - \\Psi(\\sum_{j=1}^k \\gamma_j)$$\n",
    "\n",
    "#### (2). EM Algorithm\n",
    "\n",
    "At first, the intractable log likelihood for $\\alpha$ and $\\beta$ of the data is \n",
    "\n",
    "$$\\ell(\\alpha,\\beta) = \\sum_{d=1}^M \\log p(\\textbf{w}_d|\\alpha,\\beta)$$\n",
    "\n",
    "By variational inference, the authors provide an alternative variational EM algorithm for the approximate empirical Bayes estimates for LDA model as finding the a lower bound with respect to the variational parameters $\\gamma$ and $\\phi$. The derivation of the variational EM algorithm for LDA yields the following iterative algorithm:\n",
    "  1. (E-step) For each document, find the optimizing values of the variational parameters $\\{\\gamma^*_d , \\phi^*_d : d \\in D\\}$. This is done as described in the previous section.\n",
    "  2. (M-step) Maximize the resulting lower bound on the log likelihood with respect to the model parameters $\\alpha$ and $\\beta$. This corresponds to finding maximum likelihood estimates with expected sufficient statistics for each document under the approximate posterior which is computed in the E-step.\n",
    "  3. Repeat all these two steps until the lower bound on the log likelihood converges.\n",
    "\n",
    "By this procedure, the estimate of $\\beta$ is\n",
    "\n",
    "$$\\beta_{ij} \\propto \\sum_{d=1}^M \\sum_{n=1}^{N_d} \\phi^*_{dni} w^j_{dn}$$\n",
    "\n",
    "For the M-step of $\\alpha$, the authors implement using a Newton-Raphson method.\n",
    "\n",
    "## 3. Implementation on Real Dataset\n",
    "## 4. Optimization and Analysis\n",
    "## 5. Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
