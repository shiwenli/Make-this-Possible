{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f= open('voc', 'r')\n",
    "voc = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_vocabulary(vocab):\n",
    "    \"\"\"Parse the vocabulary set into two dictionaries, word with its index and index with its\n",
    "word. When input is null, output should be null.\"\"\"\n",
    "    type_to_index = {}\n",
    "    index_to_type = {}\n",
    "    for word in set(vocab):\n",
    "        index_to_type[len(index_to_type)] = word\n",
    "        type_to_index[word] = len(type_to_index)        \n",
    "    return (type_to_index, index_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff = open('train', 'r')\n",
    "train = ff.read().strip().split(\"\\n\")\n",
    "fff = open('test', 'r')\n",
    "test = fff.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_data(corpus):\n",
    "    \"\"\"Parse the corpus into a list of documents. Find out the index of each word in each document and count its number of appearance in each document. When input is null, output should be null.\"\"\"\n",
    "    word_ids = []\n",
    "    word_cts = []     \n",
    "    for document_line in corpus:\n",
    "        document_word_dict = {}\n",
    "        for token in document_line.split():\n",
    "            if token not in type_to_index:\n",
    "                continue                \n",
    "            type_id = type_to_index[token]\n",
    "            if type_id not in document_word_dict:\n",
    "                document_word_dict[type_id] = 0\n",
    "            document_word_dict[type_id] += 1\n",
    "\n",
    "        word_ids.append(np.array(list(document_word_dict.keys())))\n",
    "        word_cts.append(np.array(list(document_word_dict.values()))[np.newaxis, :])\n",
    "\n",
    "    return (word_ids, word_cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dirichlet_expectation(dirichlet_parameter):\n",
    "    \"\"\"Calculate the expectation of dirichlet parameter. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(dirichlet_parameter).size:\n",
    "        return (\"The dirichlet_parameter is null.\")\n",
    "    if (len(dirichlet_parameter.shape) == 1):\n",
    "        return (sp.special.psi(dirichlet_parameter)-sp.special.psi(np.sum(dirichlet_parameter)))\n",
    "    return (sp.special.psi(dirichlet_parameter) - sp.special.psi(np.sum(dirichlet_parameter, 1))[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests of function parse_vocabulary, parse_data, and compute_dirichlet_expectation, for both common and edge cases. Test results are consistent with the expectations which are described in docstrings.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'govern': 1,\n",
       "  'million': 7,\n",
       "  'new': 5,\n",
       "  'peopl': 6,\n",
       "  'percent': 0,\n",
       "  'presid': 3,\n",
       "  'report': 8,\n",
       "  'state': 4,\n",
       "  'year': 2},\n",
       " {0: 'percent',\n",
       "  1: 'govern',\n",
       "  2: 'year',\n",
       "  3: 'presid',\n",
       "  4: 'state',\n",
       "  5: 'new',\n",
       "  6: 'peopl',\n",
       "  7: 'million',\n",
       "  8: 'report'})"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_test = [\"year\",\"state\",\"new\",\"percent\",\"peopl\",\"report\",\"million\",\"govern\",\"presid\"]\n",
    "parse_vocabulary(voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_test = []\n",
    "parse_vocabulary(voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([1795, 1313, 6153, 5515, 4301, 1038, 5075, 1877, 3222, 1303, 3292,\n",
       "         5342, 1439, 4193, 4391, 3504, 5812, 2398,  905, 2426, 6460, 2133]),\n",
       "  array([1729, 6343, 6153, 5047, 4556, 2062, 1359, 2705, 6035, 1303, 1885,\n",
       "         3622, 3432, 4009, 3948, 2799, 3635, 2098, 4723, 5812, 1654, 5111,\n",
       "         1465]),\n",
       "  array([2759, 6153, 1163, 3725, 2062,  973, 1303, 1307, 3292, 3748, 3622,\n",
       "         1329, 3432, 3948,  882, 4974, 3773,  905, 3761, 1202, 4723, 1654,\n",
       "         5807, 2234, 4093])],\n",
       " [array([[1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 7, 1, 1, 2, 1, 1, 1, 1, 3, 2, 2, 1]]),\n",
       "  array([[4, 1, 3, 1, 1, 1, 1, 2, 1, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1]]),\n",
       "  array([[1, 2, 1, 1, 2, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 2, 1]])])"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = [\"mauric adult adult peopl year h last year resolv polic polic polic polic polic polic polic motiv appear cri christian christian three troubl pride farley farley friday lock bureau gun gun gun church marino marino\",\"work posit reach reach peopl year thought million million million presid govern year year first percent state appear abl time billion told resid complet day develop made made made made wednesday wednesday\",\"work offer shoot thought million far year year polic polic polic polic polic polic polic polic polic cash appear abl sophist told told woman woman arrest retain found day monday citi administr prepar fame art gun\"]\n",
    "parse_data(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = []\n",
    "parse_data(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-6816.20562896,   -12.20339424, -6816.20562896, ...,\n",
       "          -11.37191495,    -9.69662577,   -27.3306072 ],\n",
       "       [-6814.90096574, -6814.90096574, -6814.90096574, ...,\n",
       "        -6814.90096574,    -9.18644145,    -8.92876909],\n",
       "       [-6814.98827358, -6814.98827358, -6814.98827358, ...,\n",
       "        -6814.98827358,    -9.31237134,    -8.30870207],\n",
       "       ..., \n",
       "       [-6815.34267505,   -10.7311724 , -6815.34267505, ...,\n",
       "        -6815.34267505,    -9.13004649,   -58.52477991],\n",
       "       [  -10.82413382,    -9.15102217, -6816.06867703, ...,\n",
       "          -40.17127912,   -10.34264717,   -10.93796738],\n",
       "       [-6815.55863679,   -14.48155799, -6815.55863679, ...,\n",
       "          -18.86407708,    -9.22004585,   -29.5117594 ]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dirichlet_expectation(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dirichlet_parameter is null.'"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_test = []\n",
    "compute_dirichlet_expectation(eta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_step(corpus=None,local_parameter_iteration=50,\n",
    "local_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"E step. When input is None, output should be document_log_likelihood, phi_sufficient_statistics, and gamma. Otherwise, it should be words_log_likelihood, gamma_values.\"\"\"\n",
    "    if corpus == None:\n",
    "        word_ids = parsed_corpus[0]\n",
    "        word_cts = parsed_corpus[1]\n",
    "    else:\n",
    "        word_ids = corpus[0]\n",
    "        word_cts = corpus[1]\n",
    "    # Initialization \n",
    "    number_of_documents = len(word_ids)\n",
    "    document_log_likelihood = 0\n",
    "    words_log_likelihood = 0\n",
    "    phi_sufficient_statistics = np.zeros((number_of_topics, number_of_types))\n",
    "    gamma_values = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics\n",
    "    E_log_eta = compute_dirichlet_expectation(eta)\n",
    "    if parsed_corpus != None:\n",
    "        E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # iterate over all documents\n",
    "    for doc_id in np.random.permutation(number_of_documents):\n",
    "        # compute the total number of words\n",
    "        total_word_count = np.sum(word_cts[doc_id])\n",
    "        # initialize gamma for this document\n",
    "        gamma_values[doc_id, :] = alpha_alpha + 1.0 * total_word_count / number_of_topics\n",
    "\n",
    "        term_ids = word_ids[doc_id]\n",
    "        term_counts = word_cts[doc_id]\n",
    "\n",
    "        # update phi and gamma until gamma converges\n",
    "        for gamma_iteration in range(local_parameter_iteration):\n",
    "            log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[doc_id, :]), (word_ids[doc_id].shape[0], 1))\n",
    "            log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis]\n",
    "            gamma_update = alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(np.repeat(term_counts, number_of_topics, axis=0).T)), axis=0))\n",
    "            mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]))\n",
    "            gamma_values[doc_id, :] = gamma_update\n",
    "            if mean_change <= local_parameter_converge_threshold:\n",
    "                break\n",
    "\n",
    "        # compute the alpha, gamma, and phi terms\n",
    "        document_log_likelihood += sp.special.gammaln(np.sum(alpha_alpha)) - np.sum(sp.special.gammaln(alpha_alpha))\n",
    "        document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]))\n",
    "        document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi))\n",
    "\n",
    "# compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step\n",
    "        words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids])      \n",
    "        phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T\n",
    "        \n",
    "    if corpus == None:\n",
    "        gamma = gamma_values\n",
    "        return (document_log_likelihood, phi_sufficient_statistics, gamma)\n",
    "    else:\n",
    "        return (words_log_likelihood, gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def m_step(phi_sufficient_statistics):\n",
    "    \"\"\"M step. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(phi_sufficient_statistics).size:\n",
    "        return (\"The input is null.\")\n",
    "    # compute the beta and the eta terms\n",
    "    topic_log_likelihood = number_of_topics * (sp.special.gammaln(np.sum(alpha_beta)) - np.sum(sp.special.gammaln(alpha_beta)))\n",
    "    topic_log_likelihood += np.sum(np.sum(sp.special.gammaln(eta), axis=1) - sp.special.gammaln(np.sum(eta, axis=1)))\n",
    "\n",
    "    eta_temp = phi_sufficient_statistics + alpha_beta\n",
    "\n",
    "    # compute the sufficient statistics for alpha and update\n",
    "    alpha_sufficient_statistics = sp.special.psi(gamma) - sp.special.psi(np.sum(gamma, axis=1)[:, np.newaxis])\n",
    "    alpha_sufficient_statistics = np.sum(alpha_sufficient_statistics, axis=0)  \n",
    "\n",
    "    return (topic_log_likelihood, alpha_sufficient_statistics, eta_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "type_to_index = parse_vocabulary(voc)[0]\n",
    "index_to_type = parse_vocabulary(voc)[1]\n",
    "parsed_corpus = parse_data(train)\n",
    "test_corpus = parse_data(test)\n",
    "\n",
    "number_of_topics = 100\n",
    "number_of_types = len(type_to_index)\n",
    "number_of_documents = len(parsed_corpus[0])\n",
    "\n",
    "alpha_alpha = np.zeros(number_of_topics) + 1/number_of_topics\n",
    "alpha_beta = np.zeros(number_of_types) + 1/number_of_types\n",
    "eta = np.random.gamma(100., 1./100., (number_of_topics,number_of_types))\n",
    "gamma = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics       \n",
    "\n",
    "hyper_parameter_optimize_interval=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute likelihood and sufficient statistics. Also serve as a testing when input of e_step is null. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing of function m_step and implement M step to fit the model.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The input is null.'"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_log_likelihood, alpha_sufficient_statistics, eta = m_step(phi_sufficient_statistics)\n",
    "phi_sufficient_statistics_test = []\n",
    "m_step(phi_sufficient_statistics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joint_log_likelihood = document_log_likelihood + topic_log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(alpha_sufficient_statistics, hyper_parameter_iteration=100, hyper_parameter_decay_factor=0.9, hyper_parameter_maximum_decay=10,alpha=alpha_alpha, hyper_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"Optimize hyperparameter alpha. Since the function's input is the result of m_step function, it is unnecessary and difficult to test it. Just use it.\"\"\"\n",
    "    alpha_update = alpha        \n",
    "    decay = 0\n",
    "    for alpha_iteration in range(hyper_parameter_iteration):\n",
    "        alpha_gradient = number_of_documents * (sp.special.psi(np.sum(alpha)) - sp.special.psi(alpha)) + alpha_sufficient_statistics\n",
    "        alpha_hessian = -number_of_documents * sp.special.polygamma(1,alpha)\n",
    "\n",
    "        sum_g_h = np.sum(alpha_gradient / alpha_hessian)\n",
    "        sum_1_h = 1.0 / alpha_hessian\n",
    "        z = number_of_documents * sp.special.polygamma(1, np.sum(alpha))\n",
    "        c = sum_g_h / (1.0 / z + sum_1_h)\n",
    "\n",
    "        # update the alpha vector\n",
    "        while True:\n",
    "            singular_hessian = False\n",
    "            step_size = np.power(hyper_parameter_decay_factor, decay) * (alpha_gradient - c) / alpha_hessian               \n",
    "            if np.any(alpha <= step_size):\n",
    "                singular_hessian = True\n",
    "            else: alpha_update = alpha - step_size\n",
    "\n",
    "            if singular_hessian:\n",
    "                decay += 1\n",
    "                if decay > hyper_parameter_maximum_decay:\n",
    "                    break\n",
    "            else: break\n",
    "\n",
    "        # check the alpha converge criteria\n",
    "        mean_change = np.mean(abs(alpha_update - alpha))\n",
    "        alpha = alpha_update\n",
    "        if mean_change <= hyper_parameter_converge_threshold:\n",
    "            break\n",
    "\n",
    "    return (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_alpha = optimize_hyperparameters(alpha_sufficient_statistics)\n",
    "words_log_likelihood, corpus_gamma_values = e_step(corpus = test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every topic, find out the words that are most likely to appear in this topic. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E_log_eta = compute_dirichlet_expectation(eta)\n",
    "topic = []\n",
    "for topic_index in range(number_of_topics):\n",
    "    temp_list = []\n",
    "    beta_probability = np.exp(E_log_eta[topic_index, :] - sp.misc.logsumexp(E_log_eta[topic_index, :]))\n",
    "    i = 0\n",
    "    for type_index in reversed(np.argsort(beta_probability)):\n",
    "        i += 1\n",
    "        if i <= 30:\n",
    "            temp_list.append(index_to_type[type_index])\n",
    "        else: break\n",
    "    topic.append(temp_list)\n",
    "\n",
    "index = random.sample(range(number_of_topics),10)\n",
    "df = pd.DataFrame({\"topic0\":topic[index[9]],\"topic1\":topic[index[0]],\"topic2\":topic[index[1]],\"topic3\":topic[index[2]],\"topic4\":topic[index[3]],\"topic5\":topic[index[4]],\"topic6\":topic[index[5]],\"topic7\":topic[index[6]],\"topic8\":topic[index[7]],\"topic9\":topic[index[8]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>contract</td>\n",
       "      <td>year</td>\n",
       "      <td>new</td>\n",
       "      <td>pilot</td>\n",
       "      <td>percent</td>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>state</td>\n",
       "      <td>say</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>year</td>\n",
       "      <td>eastern</td>\n",
       "      <td>year</td>\n",
       "      <td>million</td>\n",
       "      <td>polic</td>\n",
       "      <td>year</td>\n",
       "      <td>polic</td>\n",
       "      <td>percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>presid</td>\n",
       "      <td>report</td>\n",
       "      <td>market</td>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>percent</td>\n",
       "      <td>peopl</td>\n",
       "      <td>court</td>\n",
       "      <td>new</td>\n",
       "      <td>presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>govern</td>\n",
       "      <td>new</td>\n",
       "      <td>stock</td>\n",
       "      <td>judg</td>\n",
       "      <td>offici</td>\n",
       "      <td>two</td>\n",
       "      <td>say</td>\n",
       "      <td>two</td>\n",
       "      <td>rose</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>month</td>\n",
       "      <td>offici</td>\n",
       "      <td>price</td>\n",
       "      <td>feder</td>\n",
       "      <td>new</td>\n",
       "      <td>meet</td>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "      <td>two</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new</td>\n",
       "      <td>wine</td>\n",
       "      <td>nation</td>\n",
       "      <td>day</td>\n",
       "      <td>last</td>\n",
       "      <td>month</td>\n",
       "      <td>two</td>\n",
       "      <td>judg</td>\n",
       "      <td>meet</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>compani</td>\n",
       "      <td>water</td>\n",
       "      <td>million</td>\n",
       "      <td>attorney</td>\n",
       "      <td>unit</td>\n",
       "      <td>report</td>\n",
       "      <td>million</td>\n",
       "      <td>say</td>\n",
       "      <td>percent</td>\n",
       "      <td>dukaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ford</td>\n",
       "      <td>feeney</td>\n",
       "      <td>offici</td>\n",
       "      <td>week</td>\n",
       "      <td>peopl</td>\n",
       "      <td>fbi</td>\n",
       "      <td>fire</td>\n",
       "      <td>vote</td>\n",
       "      <td>parent</td>\n",
       "      <td>congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>offici</td>\n",
       "      <td>nation</td>\n",
       "      <td>dollar</td>\n",
       "      <td>time</td>\n",
       "      <td>sale</td>\n",
       "      <td>agent</td>\n",
       "      <td>unit</td>\n",
       "      <td>democrat</td>\n",
       "      <td>report</td>\n",
       "      <td>hou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>make</td>\n",
       "      <td>law</td>\n",
       "      <td>report</td>\n",
       "      <td>record</td>\n",
       "      <td>time</td>\n",
       "      <td>govern</td>\n",
       "      <td>report</td>\n",
       "      <td>soviet</td>\n",
       "      <td>year</td>\n",
       "      <td>nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>militari</td>\n",
       "      <td>feder</td>\n",
       "      <td>govern</td>\n",
       "      <td>peopl</td>\n",
       "      <td>presid</td>\n",
       "      <td>minist</td>\n",
       "      <td>wednesday</td>\n",
       "      <td>day</td>\n",
       "      <td>children</td>\n",
       "      <td>campaign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>first</td>\n",
       "      <td>weather</td>\n",
       "      <td>state</td>\n",
       "      <td>job</td>\n",
       "      <td>soviet</td>\n",
       "      <td>friday</td>\n",
       "      <td>time</td>\n",
       "      <td>percent</td>\n",
       "      <td>senat</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>chrysler</td>\n",
       "      <td>california</td>\n",
       "      <td>day</td>\n",
       "      <td>charg</td>\n",
       "      <td>bush</td>\n",
       "      <td>studi</td>\n",
       "      <td>forc</td>\n",
       "      <td>report</td>\n",
       "      <td>stock</td>\n",
       "      <td>plan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>talk</td>\n",
       "      <td>associ</td>\n",
       "      <td>work</td>\n",
       "      <td>offic</td>\n",
       "      <td>month</td>\n",
       "      <td>group</td>\n",
       "      <td>offici</td>\n",
       "      <td>polit</td>\n",
       "      <td>compani</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>polit</td>\n",
       "      <td>member</td>\n",
       "      <td>percent</td>\n",
       "      <td>two</td>\n",
       "      <td>first</td>\n",
       "      <td>last</td>\n",
       "      <td>ship</td>\n",
       "      <td>rule</td>\n",
       "      <td>deconcini</td>\n",
       "      <td>democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gener</td>\n",
       "      <td>time</td>\n",
       "      <td>two</td>\n",
       "      <td>defen</td>\n",
       "      <td>week</td>\n",
       "      <td>end</td>\n",
       "      <td>go</td>\n",
       "      <td>think</td>\n",
       "      <td>citi</td>\n",
       "      <td>farmer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>friday</td>\n",
       "      <td>philippin</td>\n",
       "      <td>court</td>\n",
       "      <td>say</td>\n",
       "      <td>prison</td>\n",
       "      <td>first</td>\n",
       "      <td>year</td>\n",
       "      <td>peopl</td>\n",
       "      <td>market</td>\n",
       "      <td>call</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>parti</td>\n",
       "      <td>product</td>\n",
       "      <td>share</td>\n",
       "      <td>plan</td>\n",
       "      <td>meet</td>\n",
       "      <td>time</td>\n",
       "      <td>nation</td>\n",
       "      <td>last</td>\n",
       "      <td>work</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>report</td>\n",
       "      <td>congress</td>\n",
       "      <td>week</td>\n",
       "      <td>call</td>\n",
       "      <td>nation</td>\n",
       "      <td>program</td>\n",
       "      <td>kill</td>\n",
       "      <td>nation</td>\n",
       "      <td>york</td>\n",
       "      <td>soviet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two</td>\n",
       "      <td>group</td>\n",
       "      <td>issu</td>\n",
       "      <td>rule</td>\n",
       "      <td>say</td>\n",
       "      <td>testimoni</td>\n",
       "      <td>work</td>\n",
       "      <td>offici</td>\n",
       "      <td>peopl</td>\n",
       "      <td>govern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>share</td>\n",
       "      <td>world</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>noriega</td>\n",
       "      <td>govern</td>\n",
       "      <td>call</td>\n",
       "      <td>call</td>\n",
       "      <td>countri</td>\n",
       "      <td>investig</td>\n",
       "      <td>gorbachev</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>last</td>\n",
       "      <td>last</td>\n",
       "      <td>board</td>\n",
       "      <td>compani</td>\n",
       "      <td>day</td>\n",
       "      <td>new</td>\n",
       "      <td>govern</td>\n",
       "      <td>hou</td>\n",
       "      <td>three</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>percent</td>\n",
       "      <td>aspirin</td>\n",
       "      <td>thursday</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>order</td>\n",
       "      <td>plane</td>\n",
       "      <td>three</td>\n",
       "      <td>bush</td>\n",
       "      <td>make</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>corp</td>\n",
       "      <td>yanci</td>\n",
       "      <td>exchang</td>\n",
       "      <td>air</td>\n",
       "      <td>increa</td>\n",
       "      <td>hispan</td>\n",
       "      <td>troop</td>\n",
       "      <td>first</td>\n",
       "      <td>polit</td>\n",
       "      <td>today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>today</td>\n",
       "      <td>militari</td>\n",
       "      <td>trade</td>\n",
       "      <td>train</td>\n",
       "      <td>product</td>\n",
       "      <td>testifi</td>\n",
       "      <td>presid</td>\n",
       "      <td>law</td>\n",
       "      <td>court</td>\n",
       "      <td>countri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>stock</td>\n",
       "      <td>repr</td>\n",
       "      <td>order</td>\n",
       "      <td>million</td>\n",
       "      <td>charg</td>\n",
       "      <td>peopl</td>\n",
       "      <td>militari</td>\n",
       "      <td>go</td>\n",
       "      <td>committ</td>\n",
       "      <td>compani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>time</td>\n",
       "      <td>warn</td>\n",
       "      <td>offic</td>\n",
       "      <td>case</td>\n",
       "      <td>jackpot</td>\n",
       "      <td>member</td>\n",
       "      <td>day</td>\n",
       "      <td>forc</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>told</td>\n",
       "      <td>director</td>\n",
       "      <td>billion</td>\n",
       "      <td>airlin</td>\n",
       "      <td>made</td>\n",
       "      <td>state</td>\n",
       "      <td>armi</td>\n",
       "      <td>govern</td>\n",
       "      <td>ask</td>\n",
       "      <td>tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>work</td>\n",
       "      <td>fire</td>\n",
       "      <td>last</td>\n",
       "      <td>strike</td>\n",
       "      <td>leader</td>\n",
       "      <td>opec</td>\n",
       "      <td>soviet</td>\n",
       "      <td>case</td>\n",
       "      <td>offici</td>\n",
       "      <td>talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>industri</td>\n",
       "      <td>statement</td>\n",
       "      <td>member</td>\n",
       "      <td>like</td>\n",
       "      <td>monday</td>\n",
       "      <td>judg</td>\n",
       "      <td>mr</td>\n",
       "      <td>right</td>\n",
       "      <td>kill</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic0      topic1    topic2    topic3   topic4     topic5     topic6  \\\n",
       "0   contract        year       new     pilot  percent       year      state   \n",
       "1       year       state      year   eastern     year    million      polic   \n",
       "2     presid      report    market      year    state    percent      peopl   \n",
       "3     govern         new     stock      judg   offici        two        say   \n",
       "4      month      offici     price     feder      new       meet        new   \n",
       "5        new        wine    nation       day     last      month        two   \n",
       "6    compani       water   million  attorney     unit     report    million   \n",
       "7       ford      feeney    offici      week    peopl        fbi       fire   \n",
       "8     offici      nation    dollar      time     sale      agent       unit   \n",
       "9       make         law    report    record     time     govern     report   \n",
       "10  militari       feder    govern     peopl   presid     minist  wednesday   \n",
       "11     first     weather     state       job   soviet     friday       time   \n",
       "12  chrysler  california       day     charg     bush      studi       forc   \n",
       "13      talk      associ      work     offic    month      group     offici   \n",
       "14     polit      member   percent       two    first       last       ship   \n",
       "15     gener        time       two     defen     week        end         go   \n",
       "16    friday   philippin     court       say   prison      first       year   \n",
       "17     parti     product     share      plan     meet       time     nation   \n",
       "18    report    congress      week      call   nation    program       kill   \n",
       "19       two       group      issu      rule      say  testimoni       work   \n",
       "20     share       world   tuesday   noriega   govern       call       call   \n",
       "21      last        last     board   compani      day        new     govern   \n",
       "22   percent     aspirin  thursday   tuesday    order      plane      three   \n",
       "23      corp       yanci   exchang       air   increa     hispan      troop   \n",
       "24     today    militari     trade     train  product    testifi     presid   \n",
       "25     stock        repr     order   million    charg      peopl   militari   \n",
       "26      time        warn     offic      case  jackpot     member        day   \n",
       "27      told    director   billion    airlin     made      state       armi   \n",
       "28      work        fire      last    strike   leader       opec     soviet   \n",
       "29  industri   statement    member      like   monday       judg         mr   \n",
       "\n",
       "      topic7     topic8     topic9  \n",
       "0      state        say       bush  \n",
       "1       year      polic    percent  \n",
       "2      court        new     presid  \n",
       "3        two       rose       year  \n",
       "4        new        two      state  \n",
       "5       judg       meet        say  \n",
       "6        say    percent     dukaki  \n",
       "7       vote     parent   congress  \n",
       "8   democrat     report        hou  \n",
       "9     soviet       year     nation  \n",
       "10       day   children   campaign  \n",
       "11   percent      senat       last  \n",
       "12    report      stock       plan  \n",
       "13     polit    compani      peopl  \n",
       "14      rule  deconcini   democrat  \n",
       "15     think       citi     farmer  \n",
       "16     peopl     market       call  \n",
       "17      last       work        two  \n",
       "18    nation       york     soviet  \n",
       "19    offici      peopl     govern  \n",
       "20   countri   investig  gorbachev  \n",
       "21       hou      three       work  \n",
       "22      bush       make       time  \n",
       "23     first      polit      today  \n",
       "24       law      court    countri  \n",
       "25        go    committ    compani  \n",
       "26      forc    tuesday        new  \n",
       "27    govern        ask    tuesday  \n",
       "28      case     offici       talk  \n",
       "29     right       kill      white  "
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
