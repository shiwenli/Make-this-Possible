{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f= open('voc', 'r')\n",
    "voc = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_vocabulary(vocab):\n",
    "    \"\"\"Parse the vocabulary set into two dictionaries, word with its index and index with its\n",
    "word. When input is null, output should be null.\"\"\"\n",
    "    type_to_index = {}\n",
    "    index_to_type = {}\n",
    "    for word in set(vocab):\n",
    "        index_to_type[len(index_to_type)] = word\n",
    "        type_to_index[word] = len(type_to_index)        \n",
    "    return (type_to_index, index_to_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# true data\n",
    "type_to_index = parse_vocabulary(voc)[0]\n",
    "index_to_type = parse_vocabulary(voc)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ff = open('train', 'r')\n",
    "train = ff.read().strip().split(\"\\n\")\n",
    "fff = open('test', 'r')\n",
    "test = fff.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_data(corpus):\n",
    "    \"\"\"Parse the corpus into a list of documents. Find out the index of each word in each document and count its number of appearance in each document. When input is null, output should be null.\"\"\"\n",
    "    word_ids = []\n",
    "    word_cts = []     \n",
    "    for document_line in corpus:\n",
    "        document_word_dict = {}\n",
    "        for token in document_line.split():\n",
    "            if token not in type_to_index:\n",
    "                continue                \n",
    "            type_id = type_to_index[token]\n",
    "            if type_id not in document_word_dict:\n",
    "                document_word_dict[type_id] = 0\n",
    "            document_word_dict[type_id] += 1\n",
    "\n",
    "        word_ids.append(np.array(list(document_word_dict.keys())))\n",
    "        word_cts.append(np.array(list(document_word_dict.values()))[np.newaxis, :])\n",
    "\n",
    "    return (word_ids, word_cts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dirichlet_expectation(dirichlet_parameter):\n",
    "    \"\"\"Calculate the expectation of dirichlet parameter. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(dirichlet_parameter).size:\n",
    "        return (\"The dirichlet_parameter is null.\")\n",
    "    if (len(dirichlet_parameter.shape) == 1):\n",
    "        return (sp.special.psi(dirichlet_parameter)-sp.special.psi(np.sum(dirichlet_parameter)))\n",
    "    return (sp.special.psi(dirichlet_parameter) - sp.special.psi(np.sum(dirichlet_parameter, 1))[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests of function parse_vocabulary, parse_data, and compute_dirichlet_expectation, for both common and edge cases. Test results are consistent with the expectations which are described in docstrings.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'govern': 6,\n",
       "  'million': 4,\n",
       "  'new': 8,\n",
       "  'peopl': 1,\n",
       "  'percent': 7,\n",
       "  'presid': 2,\n",
       "  'report': 3,\n",
       "  'state': 0,\n",
       "  'year': 5},\n",
       " {0: 'state',\n",
       "  1: 'peopl',\n",
       "  2: 'presid',\n",
       "  3: 'report',\n",
       "  4: 'million',\n",
       "  5: 'year',\n",
       "  6: 'govern',\n",
       "  7: 'percent',\n",
       "  8: 'new'})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_test = [\"year\",\"state\",\"new\",\"percent\",\"peopl\",\"report\",\"million\",\"govern\",\"presid\"]\n",
    "parse_vocabulary(voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, {})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_test = []\n",
    "parse_vocabulary(voc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([5953, 2309, 1478, 6337, 2763, 4942, 5037, 1936,  849, 4954, 2140,\n",
       "         5658, 5471, 5672, 1351, 3309,  637, 4339, 3638, 2746, 4924,  253]),\n",
       "  array([1478, 4339, 1737, 5514, 4428, 3790,  591, 2577, 3991,  420, 2383,\n",
       "         3292, 1054, 5471, 2596, 3654,  166, 5265, 5913, 2926,  754, 1395,\n",
       "         3190]),\n",
       "  array([3872, 5573, 3654, 1737, 4942, 2383, 2128, 4945, 5710, 5460, 1109,\n",
       "           86, 3288, 3292, 5471, 4768, 4835, 2596,  582,  166, 1478, 5037,\n",
       "         3569, 1780, 4169])],\n",
       " [array([[1, 1, 1, 1, 1, 7, 3, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2]]),\n",
       "  array([[1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 3, 3, 1, 1, 1, 2, 2, 1, 1, 1]]),\n",
       "  array([[1, 1, 1, 1, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1,\n",
       "          2, 1, 1]])])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = [\"mauric adult adult peopl year h last year resolv polic polic polic polic polic polic polic motiv appear cri christian christian three troubl pride farley farley friday lock bureau gun gun gun church marino marino\",\"work posit reach reach peopl year thought million million million presid govern year year first percent state appear abl time billion told resid complet day develop made made made made wednesday wednesday\",\"work offer shoot thought million far year year polic polic polic polic polic polic polic polic polic cash appear abl sophist told told woman woman arrest retain found day monday citi administr prepar fame art gun\"]\n",
    "parse_data(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test = []\n",
    "parse_data(corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-9.52796673, -9.60434618, -9.42120346, ..., -9.44474476,\n",
       "        -9.24287867, -9.51159128],\n",
       "       [-9.240929  , -9.49762594, -9.34999158, ..., -9.08952047,\n",
       "        -9.37434663, -9.16787778],\n",
       "       [-9.11738392, -9.49026457, -9.13080326, ..., -9.86859296,\n",
       "        -9.20836237, -9.19514557],\n",
       "       ..., \n",
       "       [-9.80143616, -9.20949025, -9.42926963, ..., -9.26464589,\n",
       "        -9.17977571, -9.41576343],\n",
       "       [-9.12376768, -9.7927182 , -9.6159151 , ..., -9.43533907,\n",
       "        -9.31465568, -9.15600508],\n",
       "       [-9.4725741 , -9.17515834, -9.43266649, ..., -9.49540765,\n",
       "        -9.58461425, -9.53051858]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 100\n",
    "number_of_types = len(type_to_index)\n",
    "eta = np.random.gamma(100., 1./100., (number_of_topics,number_of_types))\n",
    "compute_dirichlet_expectation(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dirichlet_parameter is null.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_test = []\n",
    "compute_dirichlet_expectation(eta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "parsed_corpus = parse_data(train)\n",
    "test_corpus = parse_data(test)\n",
    "number_of_documents = len(parsed_corpus[0])\n",
    "\n",
    "alpha_alpha = np.zeros(number_of_topics) + 1/number_of_topics\n",
    "alpha_beta = np.zeros(number_of_types) + 1/number_of_types\n",
    "gamma = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics       \n",
    "\n",
    "hyper_parameter_optimize_interval=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_step(corpus=None,local_parameter_iteration=50,number_of_topics=number_of_topics,\n",
    "local_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"E step. When input is None, output should be document_log_likelihood, phi_sufficient_statistics, and gamma. Otherwise, it should be words_log_likelihood, gamma_values.\"\"\"\n",
    "    if corpus == None:\n",
    "        word_ids = parsed_corpus[0]\n",
    "        word_cts = parsed_corpus[1]\n",
    "    else:\n",
    "        word_ids = corpus[0]\n",
    "        word_cts = corpus[1]\n",
    "    # Initialization \n",
    "    number_of_documents = len(word_ids)\n",
    "    document_log_likelihood = 0\n",
    "    words_log_likelihood = 0\n",
    "    phi_sufficient_statistics = np.zeros((number_of_topics, number_of_types))\n",
    "    gamma_values = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics\n",
    "    E_log_eta = compute_dirichlet_expectation(eta)\n",
    "    if parsed_corpus != None:\n",
    "        E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # iterate over all documents\n",
    "    for doc_id in np.random.permutation(number_of_documents):\n",
    "        # compute the total number of words\n",
    "        total_word_count = np.sum(word_cts[doc_id])\n",
    "        # initialize gamma for this document\n",
    "        gamma_values[doc_id, :] = alpha_alpha + 1.0 * total_word_count / number_of_topics\n",
    "\n",
    "        term_ids = word_ids[doc_id]\n",
    "        term_counts = word_cts[doc_id]\n",
    "\n",
    "        # update phi and gamma until gamma converges\n",
    "        for gamma_iteration in range(local_parameter_iteration):\n",
    "            log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[doc_id, :]), (word_ids[doc_id].shape[0], 1))\n",
    "            log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis]\n",
    "            gamma_update = alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(np.repeat(term_counts, number_of_topics, axis=0).T)), axis=0))\n",
    "            mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]))\n",
    "            gamma_values[doc_id, :] = gamma_update\n",
    "            if mean_change <= local_parameter_converge_threshold:\n",
    "                break\n",
    "\n",
    "        # compute the alpha, gamma, and phi terms\n",
    "        document_log_likelihood += sp.special.gammaln(np.sum(alpha_alpha)) - np.sum(sp.special.gammaln(alpha_alpha))\n",
    "        document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]))\n",
    "        document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi))\n",
    "\n",
    "# compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step\n",
    "        words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids])      \n",
    "        phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T\n",
    "        \n",
    "    if corpus == None:\n",
    "        gamma = gamma_values\n",
    "        return (document_log_likelihood, phi_sufficient_statistics, gamma)\n",
    "    else:\n",
    "        return (words_log_likelihood, gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def m_step(phi_sufficient_statistics):\n",
    "    \"\"\"M step. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(phi_sufficient_statistics).size:\n",
    "        return (\"The input is null.\")\n",
    "    # compute the beta and the eta terms\n",
    "    topic_log_likelihood = number_of_topics * (sp.special.gammaln(np.sum(alpha_beta)) - np.sum(sp.special.gammaln(alpha_beta)))\n",
    "    topic_log_likelihood += np.sum(np.sum(sp.special.gammaln(eta), axis=1) - sp.special.gammaln(np.sum(eta, axis=1)))\n",
    "\n",
    "    eta_temp = phi_sufficient_statistics + alpha_beta\n",
    "\n",
    "    # compute the sufficient statistics for alpha and update\n",
    "    alpha_sufficient_statistics = sp.special.psi(gamma) - sp.special.psi(np.sum(gamma, axis=1)[:, np.newaxis])\n",
    "    alpha_sufficient_statistics = np.sum(alpha_sufficient_statistics, axis=0)  \n",
    "\n",
    "    return (topic_log_likelihood, alpha_sufficient_statistics, eta_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute likelihood and sufficient statistics. Also serve as a testing when input of e_step is null. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement M step to fit the model and test function m_step when input is null.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The input is null.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_log_likelihood, alpha_sufficient_statistics, eta = m_step(phi_sufficient_statistics)\n",
    "phi_sufficient_statistics_test = []\n",
    "m_step(phi_sufficient_statistics_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(alpha_sufficient_statistics, hyper_parameter_iteration=100, hyper_parameter_decay_factor=0.9, hyper_parameter_maximum_decay=10,alpha=alpha_alpha, hyper_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"Optimize hyperparameter alpha. Since the function's input is the result of m_step function, it is unnecessary and difficult to test it. Just use it.\"\"\"\n",
    "    alpha_update = alpha        \n",
    "    decay = 0\n",
    "    for alpha_iteration in range(hyper_parameter_iteration):\n",
    "        alpha_gradient = number_of_documents * (sp.special.psi(np.sum(alpha)) - sp.special.psi(alpha)) + alpha_sufficient_statistics\n",
    "        alpha_hessian = -number_of_documents * sp.special.polygamma(1,alpha)\n",
    "\n",
    "        sum_g_h = np.sum(alpha_gradient / alpha_hessian)\n",
    "        sum_1_h = 1.0 / alpha_hessian\n",
    "        z = number_of_documents * sp.special.polygamma(1, np.sum(alpha))\n",
    "        c = sum_g_h / (1.0 / z + sum_1_h)\n",
    "\n",
    "        # update the alpha vector\n",
    "        while True:\n",
    "            singular_hessian = False\n",
    "            step_size = np.power(hyper_parameter_decay_factor, decay) * (alpha_gradient - c) / alpha_hessian               \n",
    "            if np.any(alpha <= step_size):\n",
    "                singular_hessian = True\n",
    "            else: alpha_update = alpha - step_size\n",
    "\n",
    "            if singular_hessian:\n",
    "                decay += 1\n",
    "                if decay > hyper_parameter_maximum_decay:\n",
    "                    break\n",
    "            else: break\n",
    "\n",
    "        # check the alpha converge criteria\n",
    "        mean_change = np.mean(abs(alpha_update - alpha))\n",
    "        alpha = alpha_update\n",
    "        if mean_change <= hyper_parameter_converge_threshold:\n",
    "            break\n",
    "\n",
    "    return (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha_alpha = optimize_hyperparameters(alpha_sufficient_statistics)\n",
    "words_log_likelihood, corpus_gamma_values = e_step(corpus = test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For every topic, find out the top 20 words that are most likely to appear in this topic. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E_log_eta = compute_dirichlet_expectation(eta)\n",
    "topic = []\n",
    "for topic_index in range(number_of_topics):\n",
    "    temp_list = []\n",
    "    beta_probability = np.exp(E_log_eta[topic_index, :] - sp.misc.logsumexp(E_log_eta[topic_index, :]))\n",
    "    i = 0\n",
    "    for type_index in reversed(np.argsort(beta_probability)):\n",
    "        i += 1\n",
    "        if i <= 20:\n",
    "            temp_list.append(index_to_type[type_index])\n",
    "        else: break\n",
    "    topic.append(temp_list)\n",
    "\n",
    "index = random.sample(range(number_of_topics),10)\n",
    "df = pd.DataFrame({\"topic0\":topic[index[9]],\"topic1\":topic[index[0]],\"topic2\":topic[index[1]],\"topic3\":topic[index[2]],\"topic4\":topic[index[3]],\"topic5\":topic[index[4]],\"topic6\":topic[index[5]],\"topic7\":topic[index[6]],\"topic8\":topic[index[7]],\"topic9\":topic[index[8]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cent</td>\n",
       "      <td>govern</td>\n",
       "      <td>year</td>\n",
       "      <td>polic</td>\n",
       "      <td>south</td>\n",
       "      <td>year</td>\n",
       "      <td>chrysler</td>\n",
       "      <td>year</td>\n",
       "      <td>mandela</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new</td>\n",
       "      <td>school</td>\n",
       "      <td>percent</td>\n",
       "      <td>two</td>\n",
       "      <td>two</td>\n",
       "      <td>new</td>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>govern</td>\n",
       "      <td>percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>futur</td>\n",
       "      <td>new</td>\n",
       "      <td>peopl</td>\n",
       "      <td>hospit</td>\n",
       "      <td>year</td>\n",
       "      <td>peopl</td>\n",
       "      <td>compani</td>\n",
       "      <td>last</td>\n",
       "      <td>say</td>\n",
       "      <td>billion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nation</td>\n",
       "      <td>compani</td>\n",
       "      <td>forc</td>\n",
       "      <td>citi</td>\n",
       "      <td>korea</td>\n",
       "      <td>bush</td>\n",
       "      <td>presid</td>\n",
       "      <td>peopl</td>\n",
       "      <td>children</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>york</td>\n",
       "      <td>state</td>\n",
       "      <td>state</td>\n",
       "      <td>state</td>\n",
       "      <td>africa</td>\n",
       "      <td>nomin</td>\n",
       "      <td>unit</td>\n",
       "      <td>presid</td>\n",
       "      <td>report</td>\n",
       "      <td>bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>unit</td>\n",
       "      <td>day</td>\n",
       "      <td>countri</td>\n",
       "      <td>white</td>\n",
       "      <td>shell</td>\n",
       "      <td>new</td>\n",
       "      <td>relea</td>\n",
       "      <td>govern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rate</td>\n",
       "      <td>peopl</td>\n",
       "      <td>billion</td>\n",
       "      <td>custom</td>\n",
       "      <td>state</td>\n",
       "      <td>news</td>\n",
       "      <td>travel</td>\n",
       "      <td>say</td>\n",
       "      <td>presid</td>\n",
       "      <td>tax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>market</td>\n",
       "      <td>million</td>\n",
       "      <td>bush</td>\n",
       "      <td>report</td>\n",
       "      <td>million</td>\n",
       "      <td>state</td>\n",
       "      <td>million</td>\n",
       "      <td>hou</td>\n",
       "      <td>vote</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>higher</td>\n",
       "      <td>offici</td>\n",
       "      <td>countri</td>\n",
       "      <td>offici</td>\n",
       "      <td>last</td>\n",
       "      <td>issu</td>\n",
       "      <td>plant</td>\n",
       "      <td>time</td>\n",
       "      <td>nation</td>\n",
       "      <td>million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>million</td>\n",
       "      <td>presid</td>\n",
       "      <td>million</td>\n",
       "      <td>school</td>\n",
       "      <td>african</td>\n",
       "      <td>presid</td>\n",
       "      <td>motor</td>\n",
       "      <td>make</td>\n",
       "      <td>year</td>\n",
       "      <td>last</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dollar</td>\n",
       "      <td>feder</td>\n",
       "      <td>price</td>\n",
       "      <td>mile</td>\n",
       "      <td>govern</td>\n",
       "      <td>citi</td>\n",
       "      <td>last</td>\n",
       "      <td>compani</td>\n",
       "      <td>group</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>time</td>\n",
       "      <td>offic</td>\n",
       "      <td>administr</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>peopl</td>\n",
       "      <td>relea</td>\n",
       "      <td>elect</td>\n",
       "      <td>rate</td>\n",
       "      <td>elect</td>\n",
       "      <td>soviet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bush</td>\n",
       "      <td>educ</td>\n",
       "      <td>bond</td>\n",
       "      <td>friday</td>\n",
       "      <td>percent</td>\n",
       "      <td>nation</td>\n",
       "      <td>union</td>\n",
       "      <td>citi</td>\n",
       "      <td>parent</td>\n",
       "      <td>increa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>west</td>\n",
       "      <td>report</td>\n",
       "      <td>govern</td>\n",
       "      <td>offic</td>\n",
       "      <td>three</td>\n",
       "      <td>govern</td>\n",
       "      <td>corp</td>\n",
       "      <td>servic</td>\n",
       "      <td>wife</td>\n",
       "      <td>offici</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>price</td>\n",
       "      <td>day</td>\n",
       "      <td>american</td>\n",
       "      <td>peopl</td>\n",
       "      <td>offici</td>\n",
       "      <td>percent</td>\n",
       "      <td>product</td>\n",
       "      <td>week</td>\n",
       "      <td>sand</td>\n",
       "      <td>rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>fire</td>\n",
       "      <td>depart</td>\n",
       "      <td>report</td>\n",
       "      <td>food</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>talk</td>\n",
       "      <td>robert</td>\n",
       "      <td>court</td>\n",
       "      <td>black</td>\n",
       "      <td>month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>report</td>\n",
       "      <td>tuesday</td>\n",
       "      <td>new</td>\n",
       "      <td>car</td>\n",
       "      <td>compani</td>\n",
       "      <td>york</td>\n",
       "      <td>chairman</td>\n",
       "      <td>market</td>\n",
       "      <td>leader</td>\n",
       "      <td>propo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>monday</td>\n",
       "      <td>unit</td>\n",
       "      <td>point</td>\n",
       "      <td>percent</td>\n",
       "      <td>unit</td>\n",
       "      <td>soviet</td>\n",
       "      <td>first</td>\n",
       "      <td>go</td>\n",
       "      <td>nida</td>\n",
       "      <td>compani</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>lower</td>\n",
       "      <td>two</td>\n",
       "      <td>like</td>\n",
       "      <td>compani</td>\n",
       "      <td>say</td>\n",
       "      <td>day</td>\n",
       "      <td>court</td>\n",
       "      <td>work</td>\n",
       "      <td>state</td>\n",
       "      <td>industri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>two</td>\n",
       "      <td>employ</td>\n",
       "      <td>presid</td>\n",
       "      <td>new</td>\n",
       "      <td>made</td>\n",
       "      <td>report</td>\n",
       "      <td>power</td>\n",
       "      <td>day</td>\n",
       "      <td>polic</td>\n",
       "      <td>agenc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic0   topic1     topic2   topic3   topic4   topic5    topic6   topic7  \\\n",
       "0      cent   govern       year    polic    south     year  chrysler     year   \n",
       "1       new   school    percent      two      two      new      year    state   \n",
       "2     futur      new      peopl   hospit     year    peopl   compani     last   \n",
       "3    nation  compani       forc     citi    korea     bush    presid    peopl   \n",
       "4      york    state      state    state   africa    nomin      unit   presid   \n",
       "5      year     year       unit      day  countri    white     shell      new   \n",
       "6      rate    peopl    billion   custom    state     news    travel      say   \n",
       "7    market  million       bush   report  million    state   million      hou   \n",
       "8    higher   offici    countri   offici     last     issu     plant     time   \n",
       "9   million   presid    million   school  african   presid     motor     make   \n",
       "10   dollar    feder      price     mile   govern     citi      last  compani   \n",
       "11     time    offic  administr  tuesday    peopl    relea     elect     rate   \n",
       "12     bush     educ       bond   friday  percent   nation     union     citi   \n",
       "13     west   report     govern    offic    three   govern      corp   servic   \n",
       "14    price      day   american    peopl   offici  percent   product     week   \n",
       "15     fire   depart     report     food  tuesday     talk    robert    court   \n",
       "16   report  tuesday        new      car  compani     york  chairman   market   \n",
       "17   monday     unit      point  percent     unit   soviet     first       go   \n",
       "18    lower      two       like  compani      say      day     court     work   \n",
       "19      two   employ     presid      new     made   report     power      day   \n",
       "\n",
       "      topic8    topic9  \n",
       "0    mandela      year  \n",
       "1     govern   percent  \n",
       "2        say   billion  \n",
       "3   children    report  \n",
       "4     report      bank  \n",
       "5      relea    govern  \n",
       "6     presid       tax  \n",
       "7       vote     state  \n",
       "8     nation   million  \n",
       "9       year      last  \n",
       "10     group       new  \n",
       "11     elect    soviet  \n",
       "12    parent    increa  \n",
       "13      wife    offici  \n",
       "14      sand      rate  \n",
       "15     black     month  \n",
       "16    leader     propo  \n",
       "17      nida   compani  \n",
       "18     state  industri  \n",
       "19     polic     agenc  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the number of topics and compare the results.###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The reason why the following code is not wrapped into a function is that many global variables are used and there will be conflicts if putting it in a function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>two</td>\n",
       "      <td>year</td>\n",
       "      <td>govern</td>\n",
       "      <td>percent</td>\n",
       "      <td>state</td>\n",
       "      <td>year</td>\n",
       "      <td>million</td>\n",
       "      <td>year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>percent</td>\n",
       "      <td>peopl</td>\n",
       "      <td>million</td>\n",
       "      <td>million</td>\n",
       "      <td>year</td>\n",
       "      <td>peopl</td>\n",
       "      <td>new</td>\n",
       "      <td>percent</td>\n",
       "      <td>year</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>last</td>\n",
       "      <td>million</td>\n",
       "      <td>presid</td>\n",
       "      <td>peopl</td>\n",
       "      <td>state</td>\n",
       "      <td>year</td>\n",
       "      <td>year</td>\n",
       "      <td>state</td>\n",
       "      <td>percent</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>time</td>\n",
       "      <td>state</td>\n",
       "      <td>new</td>\n",
       "      <td>new</td>\n",
       "      <td>parti</td>\n",
       "      <td>polic</td>\n",
       "      <td>report</td>\n",
       "      <td>new</td>\n",
       "      <td>peopl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>state</td>\n",
       "      <td>report</td>\n",
       "      <td>say</td>\n",
       "      <td>two</td>\n",
       "      <td>percent</td>\n",
       "      <td>million</td>\n",
       "      <td>nation</td>\n",
       "      <td>new</td>\n",
       "      <td>compani</td>\n",
       "      <td>offici</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>govern</td>\n",
       "      <td>govern</td>\n",
       "      <td>year</td>\n",
       "      <td>report</td>\n",
       "      <td>say</td>\n",
       "      <td>govern</td>\n",
       "      <td>peopl</td>\n",
       "      <td>two</td>\n",
       "      <td>presid</td>\n",
       "      <td>hou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>member</td>\n",
       "      <td>two</td>\n",
       "      <td>new</td>\n",
       "      <td>soviet</td>\n",
       "      <td>presid</td>\n",
       "      <td>new</td>\n",
       "      <td>presid</td>\n",
       "      <td>nation</td>\n",
       "      <td>last</td>\n",
       "      <td>presid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nation</td>\n",
       "      <td>year</td>\n",
       "      <td>offici</td>\n",
       "      <td>polic</td>\n",
       "      <td>offici</td>\n",
       "      <td>say</td>\n",
       "      <td>two</td>\n",
       "      <td>last</td>\n",
       "      <td>time</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>offici</td>\n",
       "      <td>presid</td>\n",
       "      <td>feder</td>\n",
       "      <td>week</td>\n",
       "      <td>bush</td>\n",
       "      <td>report</td>\n",
       "      <td>last</td>\n",
       "      <td>say</td>\n",
       "      <td>price</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>month</td>\n",
       "      <td>nation</td>\n",
       "      <td>compani</td>\n",
       "      <td>govern</td>\n",
       "      <td>report</td>\n",
       "      <td>offici</td>\n",
       "      <td>day</td>\n",
       "      <td>peopl</td>\n",
       "      <td>first</td>\n",
       "      <td>soviet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>peopl</td>\n",
       "      <td>compani</td>\n",
       "      <td>last</td>\n",
       "      <td>percent</td>\n",
       "      <td>peopl</td>\n",
       "      <td>soviet</td>\n",
       "      <td>offici</td>\n",
       "      <td>offici</td>\n",
       "      <td>state</td>\n",
       "      <td>day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>report</td>\n",
       "      <td>countri</td>\n",
       "      <td>report</td>\n",
       "      <td>day</td>\n",
       "      <td>unit</td>\n",
       "      <td>time</td>\n",
       "      <td>say</td>\n",
       "      <td>today</td>\n",
       "      <td>say</td>\n",
       "      <td>offic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>polic</td>\n",
       "      <td>week</td>\n",
       "      <td>work</td>\n",
       "      <td>presid</td>\n",
       "      <td>nation</td>\n",
       "      <td>bush</td>\n",
       "      <td>soviet</td>\n",
       "      <td>soviet</td>\n",
       "      <td>stock</td>\n",
       "      <td>million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>rate</td>\n",
       "      <td>new</td>\n",
       "      <td>first</td>\n",
       "      <td>last</td>\n",
       "      <td>two</td>\n",
       "      <td>polic</td>\n",
       "      <td>report</td>\n",
       "      <td>presid</td>\n",
       "      <td>unit</td>\n",
       "      <td>percent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>presid</td>\n",
       "      <td>day</td>\n",
       "      <td>nation</td>\n",
       "      <td>say</td>\n",
       "      <td>million</td>\n",
       "      <td>monday</td>\n",
       "      <td>trade</td>\n",
       "      <td>time</td>\n",
       "      <td>report</td>\n",
       "      <td>bush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>citi</td>\n",
       "      <td>unit</td>\n",
       "      <td>member</td>\n",
       "      <td>hou</td>\n",
       "      <td>last</td>\n",
       "      <td>presid</td>\n",
       "      <td>thursday</td>\n",
       "      <td>first</td>\n",
       "      <td>day</td>\n",
       "      <td>nation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>first</td>\n",
       "      <td>american</td>\n",
       "      <td>offic</td>\n",
       "      <td>time</td>\n",
       "      <td>forc</td>\n",
       "      <td>nation</td>\n",
       "      <td>made</td>\n",
       "      <td>govern</td>\n",
       "      <td>york</td>\n",
       "      <td>congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>say</td>\n",
       "      <td>last</td>\n",
       "      <td>north</td>\n",
       "      <td>compani</td>\n",
       "      <td>leader</td>\n",
       "      <td>countri</td>\n",
       "      <td>forc</td>\n",
       "      <td>three</td>\n",
       "      <td>two</td>\n",
       "      <td>say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>work</td>\n",
       "      <td>soviet</td>\n",
       "      <td>peopl</td>\n",
       "      <td>work</td>\n",
       "      <td>polic</td>\n",
       "      <td>last</td>\n",
       "      <td>first</td>\n",
       "      <td>billion</td>\n",
       "      <td>market</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>million</td>\n",
       "      <td>offici</td>\n",
       "      <td>time</td>\n",
       "      <td>state</td>\n",
       "      <td>soviet</td>\n",
       "      <td>jackson</td>\n",
       "      <td>court</td>\n",
       "      <td>price</td>\n",
       "      <td>bush</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     topic0    topic1   topic2   topic3   topic4   topic5    topic6   topic7  \\\n",
       "0      year     state      two     year   govern  percent     state     year   \n",
       "1   percent     peopl  million  million     year    peopl       new  percent   \n",
       "2      last   million   presid    peopl    state     year      year    state   \n",
       "3       new      time    state      new      new    parti     polic   report   \n",
       "4     state    report      say      two  percent  million    nation      new   \n",
       "5    govern    govern     year   report      say   govern     peopl      two   \n",
       "6    member       two      new   soviet   presid      new    presid   nation   \n",
       "7    nation      year   offici    polic   offici      say       two     last   \n",
       "8    offici    presid    feder     week     bush   report      last      say   \n",
       "9     month    nation  compani   govern   report   offici       day    peopl   \n",
       "10    peopl   compani     last  percent    peopl   soviet    offici   offici   \n",
       "11   report   countri   report      day     unit     time       say    today   \n",
       "12    polic      week     work   presid   nation     bush    soviet   soviet   \n",
       "13     rate       new    first     last      two    polic    report   presid   \n",
       "14   presid       day   nation      say  million   monday     trade     time   \n",
       "15     citi      unit   member      hou     last   presid  thursday    first   \n",
       "16    first  american    offic     time     forc   nation      made   govern   \n",
       "17      say      last    north  compani   leader  countri      forc    three   \n",
       "18     work    soviet    peopl     work    polic     last     first  billion   \n",
       "19  million    offici     time    state   soviet  jackson     court    price   \n",
       "\n",
       "     topic8    topic9  \n",
       "0   million      year  \n",
       "1      year    report  \n",
       "2   percent     state  \n",
       "3       new     peopl  \n",
       "4   compani    offici  \n",
       "5    presid       hou  \n",
       "6      last    presid  \n",
       "7      time       new  \n",
       "8     price      time  \n",
       "9     first    soviet  \n",
       "10    state       day  \n",
       "11      say     offic  \n",
       "12    stock   million  \n",
       "13     unit   percent  \n",
       "14   report      bush  \n",
       "15      day    nation  \n",
       "16     york  congress  \n",
       "17      two       say  \n",
       "18   market       two  \n",
       "19     bush        go  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_topics = 10\n",
    "alpha_alpha = np.zeros(number_of_topics) + 1/number_of_topics\n",
    "alpha_beta = np.zeros(number_of_types) + 1/number_of_types\n",
    "eta = np.random.gamma(100., 1./100., (number_of_topics,number_of_types))\n",
    "gamma = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics \n",
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step(number_of_topics=number_of_topics)\n",
    "topic_log_likelihood, alpha_sufficient_statistics, eta = m_step(phi_sufficient_statistics)\n",
    "alpha_alpha = optimize_hyperparameters(alpha_sufficient_statistics,alpha=alpha_alpha)\n",
    "words_log_likelihood, corpus_gamma_values = e_step(corpus = test_corpus,\n",
    "number_of_topics=number_of_topics)\n",
    "E_log_eta = compute_dirichlet_expectation(eta)\n",
    "topic = []\n",
    "for topic_index in range(number_of_topics):\n",
    "    temp_list = []\n",
    "    beta_probability = np.exp(E_log_eta[topic_index, :] - sp.misc.logsumexp(E_log_eta[topic_index, :]))\n",
    "    i = 0\n",
    "    for type_index in reversed(np.argsort(beta_probability)):\n",
    "        i += 1\n",
    "        if i <= 20:\n",
    "            temp_list.append(index_to_type[type_index])\n",
    "        else: break\n",
    "    topic.append(temp_list)\n",
    "\n",
    "index = random.sample(range(number_of_topics),10)\n",
    "df = pd.DataFrame({\"topic0\":topic[index[9]],\"topic1\":topic[index[0]],\"topic2\":topic[index[1]],\"topic3\":topic[index[2]],\"topic4\":topic[index[3]],\"topic5\":topic[index[4]],\"topic6\":topic[index[5]],\"topic7\":topic[index[6]],\"topic8\":topic[index[7]],\"topic9\":topic[index[8]]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we just have 10 topics, we can see that words in different topics are similar. Because test corpus is not that large and there are not so many key words, the corpus is not fully divided into different topics and different topics will overlap with each other. So when the number of topics is small, those key words will appear repeatedly and the distinctions of topics are not obvious.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
