{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_dirichlet_expectation(dirichlet_parameter):\n",
    "    \"\"\"Calculate the expectation of dirichlet parameter. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(dirichlet_parameter).size:\n",
    "        return (\"The dirichlet_parameter is null.\")\n",
    "    if (len(dirichlet_parameter.shape) == 1):\n",
    "        return (sp.special.psi(dirichlet_parameter)-sp.special.psi(np.sum(dirichlet_parameter)))\n",
    "    return (sp.special.psi(dirichlet_parameter) - sp.special.psi(np.sum(dirichlet_parameter, 1))[:, np.newaxis])\n",
    "\n",
    "\n",
    "def e_step(corpus=None,local_parameter_iteration=50,\n",
    "local_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"E step. When input is None, output should be document_log_likelihood, phi_sufficient_statistics, and gamma. Otherwise, it should be words_log_likelihood, gamma_values.\"\"\"\n",
    "    if corpus == None:\n",
    "        word_ids = parsed_corpus[0]\n",
    "        word_cts = parsed_corpus[1]\n",
    "    else:\n",
    "        word_ids = corpus[0]\n",
    "        word_cts = corpus[1]\n",
    "    # Initialization \n",
    "    number_of_documents = len(word_ids)\n",
    "    document_log_likelihood = 0\n",
    "    words_log_likelihood = 0\n",
    "    phi_sufficient_statistics = np.zeros((number_of_topics, number_of_types))\n",
    "    gamma_values = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics\n",
    "    E_log_eta = compute_dirichlet_expectation(eta)\n",
    "    if parsed_corpus != None:\n",
    "        E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # iterate over all documents\n",
    "    for doc_id in np.random.permutation(number_of_documents):\n",
    "        # compute the total number of words\n",
    "        total_word_count = np.sum(word_cts[doc_id])\n",
    "        # initialize gamma for this document\n",
    "        gamma_values[doc_id, :] = alpha_alpha + 1.0 * total_word_count / number_of_topics\n",
    "\n",
    "        term_ids = word_ids[doc_id]\n",
    "        term_counts = word_cts[doc_id]\n",
    "\n",
    "        # update phi and gamma until gamma converges\n",
    "        for gamma_iteration in range(local_parameter_iteration):\n",
    "            log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[doc_id, :]), (word_ids[doc_id].shape[0], 1))\n",
    "            log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis]\n",
    "            gamma_update = alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(np.repeat(term_counts, number_of_topics, axis=0).T)), axis=0))\n",
    "            mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]))\n",
    "            gamma_values[doc_id, :] = gamma_update\n",
    "            if mean_change <= local_parameter_converge_threshold:\n",
    "                break\n",
    "\n",
    "        # compute the alpha, gamma, and phi terms\n",
    "        document_log_likelihood += sp.special.gammaln(np.sum(alpha_alpha)) - np.sum(sp.special.gammaln(alpha_alpha))\n",
    "        document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]))\n",
    "        document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi))\n",
    "\n",
    "# compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step\n",
    "        words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids])      \n",
    "        phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T\n",
    "        \n",
    "    if corpus == None:\n",
    "        gamma = gamma_values\n",
    "        return (document_log_likelihood, phi_sufficient_statistics, gamma)\n",
    "    else:\n",
    "        return (words_log_likelihood, gamma_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.4 s, sys: 583 ms, total: 52.9 s\n",
      "Wall time: 53.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def compute_dirichlet_expectation(dirichlet_parameter):\n",
    "    \"\"\"Calculate the expectation of dirichlet parameter. When input is null, output should be a warning that the input is null.\"\"\"\n",
    "    if not np.array(dirichlet_parameter).size:\n",
    "        return (\"The dirichlet_parameter is null.\")\n",
    "    if (len(dirichlet_parameter.shape) == 1):\n",
    "        return (sp.special.psi(dirichlet_parameter)-sp.special.psi(np.sum(dirichlet_parameter)))\n",
    "    return (sp.special.psi(dirichlet_parameter) - sp.special.psi(np.sum(dirichlet_parameter, 1))[:, np.newaxis])\n",
    "\n",
    "\n",
    "def e_step(corpus=None,local_parameter_iteration=50,\n",
    "local_parameter_converge_threshold=1e-6):\n",
    "    \"\"\"E step. When input is None, output should be document_log_likelihood, phi_sufficient_statistics, and gamma. Otherwise, it should be words_log_likelihood, gamma_values.\"\"\"\n",
    "    if corpus == None:\n",
    "        word_ids = parsed_corpus[0]\n",
    "        word_cts = parsed_corpus[1]\n",
    "    else:\n",
    "        word_ids = corpus[0]\n",
    "        word_cts = corpus[1]\n",
    "    # Initialization \n",
    "    number_of_documents = len(word_ids)\n",
    "    document_log_likelihood = 0\n",
    "    words_log_likelihood = 0\n",
    "    phi_sufficient_statistics = np.zeros((number_of_topics, number_of_types))\n",
    "    gamma_values = np.zeros((number_of_documents, number_of_topics)) + alpha_alpha[np.newaxis, :] + 1.0 * number_of_types / number_of_topics\n",
    "    E_log_eta = compute_dirichlet_expectation(eta)\n",
    "    if parsed_corpus != None:\n",
    "        E_log_prob_eta = E_log_eta - sp.misc.logsumexp(E_log_eta, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # iterate over all documents\n",
    "    for doc_id in np.random.permutation(number_of_documents):\n",
    "        # compute the total number of words\n",
    "        total_word_count = np.sum(word_cts[doc_id])\n",
    "        # initialize gamma for this document\n",
    "        gamma_values[doc_id, :] = alpha_alpha + 1.0 * total_word_count / number_of_topics\n",
    "\n",
    "        term_ids = word_ids[doc_id]\n",
    "        term_counts = word_cts[doc_id]\n",
    "\n",
    "        # update phi and gamma until gamma converges\n",
    "        for gamma_iteration in range(local_parameter_iteration):\n",
    "            log_phi = E_log_eta[:, term_ids].T + np.tile(sp.special.psi(gamma_values[doc_id, :]), (word_ids[doc_id].shape[0], 1))\n",
    "            log_phi -= sp.misc.logsumexp(log_phi, axis=1)[:, np.newaxis]\n",
    "            gamma_update = alpha_alpha + np.array(np.sum(np.exp(log_phi + np.log(np.repeat(term_counts, number_of_topics, axis=0).T)), axis=0))\n",
    "            mean_change = np.mean(abs(gamma_update - gamma_values[doc_id, :]))\n",
    "            gamma_values[doc_id, :] = gamma_update\n",
    "            if mean_change <= local_parameter_converge_threshold:\n",
    "                break\n",
    "\n",
    "        # compute the alpha, gamma, and phi terms\n",
    "        document_log_likelihood += sp.special.gammaln(np.sum(alpha_alpha)) - np.sum(sp.special.gammaln(alpha_alpha))\n",
    "        document_log_likelihood += np.sum(sp.special.gammaln(gamma_values[doc_id, :])) - sp.special.gammaln(np.sum(gamma_values[doc_id, :]))\n",
    "        document_log_likelihood -= np.sum(np.dot(term_counts, np.exp(log_phi) * log_phi))\n",
    "\n",
    "# compute the p(w_{dn} | z_{dn}, \\eta) terms, which will be cancelled during M-step\n",
    "        words_log_likelihood += np.sum(np.exp(log_phi.T + np.log(term_counts)) * E_log_prob_eta[:, term_ids])      \n",
    "        phi_sufficient_statistics[:, term_ids] += np.exp(log_phi + np.log(term_counts.transpose())).T\n",
    "        \n",
    "    if corpus == None:\n",
    "        gamma = gamma_values\n",
    "        return (document_log_likelihood, phi_sufficient_statistics, gamma)\n",
    "    else:\n",
    "        return (words_log_likelihood, gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.7 s, sys: 310 ms, total: 48 s\n",
      "Wall time: 48.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "document_log_likelihood, phi_sufficient_statistics, gamma = e_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
